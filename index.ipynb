{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jupyternotify\n",
    "ip = get_ipython()\n",
    "ip.register_magics(jupyternotify.JupyterNotifyMagics)\n",
    "## Run %notify to create notification for completed cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 60000 entries, 0 to 59999\n",
      "Data columns (total 2 columns):\n",
      "Product                         60000 non-null object\n",
      "Consumer complaint narrative    60000 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 937.6+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df.Product\n",
    "complaints = df['Consumer complaint narrative']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 2000)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"8b21e52c-41f0-445a-9ff3-b467adc7235d\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"8b21e52c-41f0-445a-9ff3-b467adc7235d\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results = tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "print(np.shape(one_hot_results))\n",
    "%notify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product)\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here\n",
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.9522 - acc: 0.1635 - val_loss: 1.9489 - val_acc: 0.1430\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9298 - acc: 0.1867 - val_loss: 1.9351 - val_acc: 0.1610\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9134 - acc: 0.1977 - val_loss: 1.9232 - val_acc: 0.1670\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8989 - acc: 0.2105 - val_loss: 1.9114 - val_acc: 0.1900\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8847 - acc: 0.2229 - val_loss: 1.8984 - val_acc: 0.2050\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8694 - acc: 0.2333 - val_loss: 1.8834 - val_acc: 0.2280\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8521 - acc: 0.2437 - val_loss: 1.8661 - val_acc: 0.2340\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8324 - acc: 0.2611 - val_loss: 1.8448 - val_acc: 0.2610\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8100 - acc: 0.2772 - val_loss: 1.8207 - val_acc: 0.2830\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7841 - acc: 0.3011 - val_loss: 1.7930 - val_acc: 0.3020\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.7549 - acc: 0.3273 - val_loss: 1.7617 - val_acc: 0.3320\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7220 - acc: 0.3617 - val_loss: 1.7248 - val_acc: 0.3710\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6849 - acc: 0.3996 - val_loss: 1.6860 - val_acc: 0.4220\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6442 - acc: 0.4475 - val_loss: 1.6446 - val_acc: 0.4510\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.6004 - acc: 0.4823 - val_loss: 1.5991 - val_acc: 0.4750\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5539 - acc: 0.5195 - val_loss: 1.5507 - val_acc: 0.5150\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5051 - acc: 0.5531 - val_loss: 1.5024 - val_acc: 0.5430\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4554 - acc: 0.5808 - val_loss: 1.4510 - val_acc: 0.5700\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4045 - acc: 0.6021 - val_loss: 1.4008 - val_acc: 0.5870\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3533 - acc: 0.6235 - val_loss: 1.3488 - val_acc: 0.5990\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3028 - acc: 0.6395 - val_loss: 1.2974 - val_acc: 0.6230\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2524 - acc: 0.6547 - val_loss: 1.2474 - val_acc: 0.6280\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2030 - acc: 0.6649 - val_loss: 1.1992 - val_acc: 0.6470\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1563 - acc: 0.6767 - val_loss: 1.1551 - val_acc: 0.6490\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1125 - acc: 0.6848 - val_loss: 1.1147 - val_acc: 0.6590\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0723 - acc: 0.6928 - val_loss: 1.0761 - val_acc: 0.6730\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0342 - acc: 0.7009 - val_loss: 1.0417 - val_acc: 0.6750\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9991 - acc: 0.7063 - val_loss: 1.0088 - val_acc: 0.6840\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.9666 - acc: 0.7119 - val_loss: 0.9807 - val_acc: 0.6860\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 0.9368 - acc: 0.7151 - val_loss: 0.9554 - val_acc: 0.6920\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.9094 - acc: 0.7208 - val_loss: 0.9286 - val_acc: 0.7000\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8837 - acc: 0.7240 - val_loss: 0.9065 - val_acc: 0.7020\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8601 - acc: 0.7315 - val_loss: 0.8869 - val_acc: 0.7030\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8383 - acc: 0.7369 - val_loss: 0.8698 - val_acc: 0.7000\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8186 - acc: 0.7387 - val_loss: 0.8512 - val_acc: 0.7080\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7994 - acc: 0.7457 - val_loss: 0.8355 - val_acc: 0.7200\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7821 - acc: 0.7484 - val_loss: 0.8217 - val_acc: 0.7170\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.7661 - acc: 0.7528 - val_loss: 0.8103 - val_acc: 0.7170\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7511 - acc: 0.7564 - val_loss: 0.7978 - val_acc: 0.7200\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7370 - acc: 0.7577 - val_loss: 0.7853 - val_acc: 0.7210\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7241 - acc: 0.7616 - val_loss: 0.7767 - val_acc: 0.7200\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7114 - acc: 0.7632 - val_loss: 0.7663 - val_acc: 0.7260\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6998 - acc: 0.7672 - val_loss: 0.7594 - val_acc: 0.7220\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6891 - acc: 0.7689 - val_loss: 0.7523 - val_acc: 0.7320\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6788 - acc: 0.7689 - val_loss: 0.7431 - val_acc: 0.7340\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6688 - acc: 0.7756 - val_loss: 0.7368 - val_acc: 0.7260\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6597 - acc: 0.7761 - val_loss: 0.7313 - val_acc: 0.7400\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6510 - acc: 0.7811 - val_loss: 0.7254 - val_acc: 0.7320\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6420 - acc: 0.7788 - val_loss: 0.7195 - val_acc: 0.7410\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6344 - acc: 0.7844 - val_loss: 0.7145 - val_acc: 0.7340\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6263 - acc: 0.7869 - val_loss: 0.7096 - val_acc: 0.7450\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6191 - acc: 0.7880 - val_loss: 0.7036 - val_acc: 0.7490\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6120 - acc: 0.7905 - val_loss: 0.7008 - val_acc: 0.7430\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6052 - acc: 0.7919 - val_loss: 0.6964 - val_acc: 0.7470\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.5985 - acc: 0.7937 - val_loss: 0.6947 - val_acc: 0.7520\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5925 - acc: 0.7960 - val_loss: 0.6888 - val_acc: 0.7510\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.5861 - acc: 0.7968 - val_loss: 0.6875 - val_acc: 0.7490\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5800 - acc: 0.7988 - val_loss: 0.6844 - val_acc: 0.7520\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5744 - acc: 0.7996 - val_loss: 0.6811 - val_acc: 0.7530\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5685 - acc: 0.8009 - val_loss: 0.6778 - val_acc: 0.7530\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5635 - acc: 0.8016 - val_loss: 0.6753 - val_acc: 0.7550\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.5579 - acc: 0.8051 - val_loss: 0.6748 - val_acc: 0.7520\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5529 - acc: 0.8079 - val_loss: 0.6698 - val_acc: 0.7570\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.5477 - acc: 0.8068 - val_loss: 0.6682 - val_acc: 0.7540\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5425 - acc: 0.8097 - val_loss: 0.6667 - val_acc: 0.7560\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5379 - acc: 0.8123 - val_loss: 0.6647 - val_acc: 0.7560\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5330 - acc: 0.8103 - val_loss: 0.6638 - val_acc: 0.7530\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.5288 - acc: 0.8152 - val_loss: 0.6601 - val_acc: 0.7610\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.5243 - acc: 0.8141 - val_loss: 0.6591 - val_acc: 0.7590\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.5200 - acc: 0.8168 - val_loss: 0.6584 - val_acc: 0.7590\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5153 - acc: 0.8171 - val_loss: 0.6558 - val_acc: 0.7590\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.5115 - acc: 0.8203 - val_loss: 0.6538 - val_acc: 0.7590\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5068 - acc: 0.8196 - val_loss: 0.6535 - val_acc: 0.7560\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.5030 - acc: 0.8215 - val_loss: 0.6515 - val_acc: 0.7620\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4992 - acc: 0.8236 - val_loss: 0.6515 - val_acc: 0.7580\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4949 - acc: 0.8241 - val_loss: 0.6496 - val_acc: 0.7590\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4912 - acc: 0.8257 - val_loss: 0.6484 - val_acc: 0.7590\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.4869 - acc: 0.8284 - val_loss: 0.6491 - val_acc: 0.7610\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4835 - acc: 0.8296 - val_loss: 0.6462 - val_acc: 0.7600\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4797 - acc: 0.8309 - val_loss: 0.6465 - val_acc: 0.7610\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4762 - acc: 0.8339 - val_loss: 0.6469 - val_acc: 0.7570\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4723 - acc: 0.8335 - val_loss: 0.6443 - val_acc: 0.7620\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4690 - acc: 0.8369 - val_loss: 0.6471 - val_acc: 0.7580\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4656 - acc: 0.8363 - val_loss: 0.6420 - val_acc: 0.7630\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4620 - acc: 0.8397 - val_loss: 0.6415 - val_acc: 0.7560\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4587 - acc: 0.8407 - val_loss: 0.6430 - val_acc: 0.7630\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4551 - acc: 0.8427 - val_loss: 0.6404 - val_acc: 0.7580\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4516 - acc: 0.8437 - val_loss: 0.6417 - val_acc: 0.7610\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4486 - acc: 0.8449 - val_loss: 0.6396 - val_acc: 0.7590\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.4449 - acc: 0.8488 - val_loss: 0.6398 - val_acc: 0.7580\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4421 - acc: 0.8499 - val_loss: 0.6401 - val_acc: 0.7660\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.4388 - acc: 0.8480 - val_loss: 0.6385 - val_acc: 0.7610\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4355 - acc: 0.8523 - val_loss: 0.6384 - val_acc: 0.7590\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4324 - acc: 0.8531 - val_loss: 0.6389 - val_acc: 0.7600\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4289 - acc: 0.8551 - val_loss: 0.6383 - val_acc: 0.7580\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.4265 - acc: 0.8567 - val_loss: 0.6400 - val_acc: 0.7600\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4230 - acc: 0.8559 - val_loss: 0.6368 - val_acc: 0.7620\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.4196 - acc: 0.8585 - val_loss: 0.6375 - val_acc: 0.7590\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.4174 - acc: 0.8585 - val_loss: 0.6367 - val_acc: 0.7570\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4141 - acc: 0.8597 - val_loss: 0.6362 - val_acc: 0.7580\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4113 - acc: 0.8605 - val_loss: 0.6355 - val_acc: 0.7570\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.4085 - acc: 0.8613 - val_loss: 0.6377 - val_acc: 0.7580\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4057 - acc: 0.8648 - val_loss: 0.6347 - val_acc: 0.7610\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.4026 - acc: 0.8644 - val_loss: 0.6370 - val_acc: 0.7610\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3998 - acc: 0.8660 - val_loss: 0.6354 - val_acc: 0.7590\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3969 - acc: 0.8675 - val_loss: 0.6355 - val_acc: 0.7580\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3944 - acc: 0.8699 - val_loss: 0.6346 - val_acc: 0.7540\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3916 - acc: 0.8691 - val_loss: 0.6413 - val_acc: 0.7620\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.3892 - acc: 0.8695 - val_loss: 0.6376 - val_acc: 0.7610\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.3861 - acc: 0.8721 - val_loss: 0.6373 - val_acc: 0.7590\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3832 - acc: 0.8733 - val_loss: 0.6374 - val_acc: 0.7550\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3807 - acc: 0.8759 - val_loss: 0.6353 - val_acc: 0.7570\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.3780 - acc: 0.8757 - val_loss: 0.6356 - val_acc: 0.7580\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.3754 - acc: 0.8741 - val_loss: 0.6361 - val_acc: 0.7530\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3733 - acc: 0.8771 - val_loss: 0.6364 - val_acc: 0.7560\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.3702 - acc: 0.8785 - val_loss: 0.6365 - val_acc: 0.7560\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3676 - acc: 0.8780 - val_loss: 0.6362 - val_acc: 0.7620\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.3650 - acc: 0.8803 - val_loss: 0.6368 - val_acc: 0.7550\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.3628 - acc: 0.8801 - val_loss: 0.6378 - val_acc: 0.7540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.3604 - acc: 0.8841 - val_loss: 0.6373 - val_acc: 0.7560\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"d4089c76-4066-4a86-a5ea-c13b77511ea3\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"d4089c76-4066-4a86-a5ea-c13b77511ea3\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n",
    "%notify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.35653433747291563, 0.8842666666984558]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5933426013787587, 0.7866666671435039]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.9712 - acc: 0.1452 - val_loss: 1.9578 - val_acc: 0.1590\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9424 - acc: 0.1655 - val_loss: 1.9399 - val_acc: 0.1730\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9256 - acc: 0.1876 - val_loss: 1.9271 - val_acc: 0.1820\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9112 - acc: 0.2044 - val_loss: 1.9151 - val_acc: 0.1970\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8970 - acc: 0.2201 - val_loss: 1.9017 - val_acc: 0.2110\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8814 - acc: 0.2404 - val_loss: 1.8872 - val_acc: 0.2240\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.8642 - acc: 0.2609 - val_loss: 1.8707 - val_acc: 0.2440\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8455 - acc: 0.2829 - val_loss: 1.8523 - val_acc: 0.2590\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8249 - acc: 0.3011 - val_loss: 1.8323 - val_acc: 0.2830\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8022 - acc: 0.3229 - val_loss: 1.8099 - val_acc: 0.3080\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.7769 - acc: 0.3448 - val_loss: 1.7841 - val_acc: 0.3330\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7485 - acc: 0.3673 - val_loss: 1.7570 - val_acc: 0.3510\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7172 - acc: 0.3896 - val_loss: 1.7234 - val_acc: 0.3890\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6833 - acc: 0.4173 - val_loss: 1.6888 - val_acc: 0.4060\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6454 - acc: 0.4465 - val_loss: 1.6490 - val_acc: 0.4290\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.6043 - acc: 0.4696 - val_loss: 1.6055 - val_acc: 0.4500\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5598 - acc: 0.4995 - val_loss: 1.5599 - val_acc: 0.4660\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5131 - acc: 0.5248 - val_loss: 1.5125 - val_acc: 0.4870\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.4648 - acc: 0.5536 - val_loss: 1.4645 - val_acc: 0.5180\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4160 - acc: 0.5740 - val_loss: 1.4172 - val_acc: 0.5370\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3672 - acc: 0.5965 - val_loss: 1.3686 - val_acc: 0.5610\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3194 - acc: 0.6136 - val_loss: 1.3223 - val_acc: 0.5770\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2724 - acc: 0.6309 - val_loss: 1.2771 - val_acc: 0.5950\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2275 - acc: 0.6445 - val_loss: 1.2343 - val_acc: 0.6110\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1845 - acc: 0.6593 - val_loss: 1.1965 - val_acc: 0.6310\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1438 - acc: 0.6696 - val_loss: 1.1619 - val_acc: 0.6320\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1056 - acc: 0.6820 - val_loss: 1.1251 - val_acc: 0.6390\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0693 - acc: 0.6892 - val_loss: 1.0894 - val_acc: 0.6390\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0352 - acc: 0.6964 - val_loss: 1.0597 - val_acc: 0.6500\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0031 - acc: 0.7052 - val_loss: 1.0354 - val_acc: 0.6630\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.9734 - acc: 0.7111 - val_loss: 1.0092 - val_acc: 0.6670\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9457 - acc: 0.7169 - val_loss: 0.9833 - val_acc: 0.6690\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9199 - acc: 0.7236 - val_loss: 0.9620 - val_acc: 0.6720\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8956 - acc: 0.7271 - val_loss: 0.9463 - val_acc: 0.6780\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.8735 - acc: 0.7312 - val_loss: 0.9237 - val_acc: 0.6790\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8520 - acc: 0.7345 - val_loss: 0.9069 - val_acc: 0.6780\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8320 - acc: 0.7407 - val_loss: 0.8906 - val_acc: 0.6940\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8140 - acc: 0.7457 - val_loss: 0.8756 - val_acc: 0.6920\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7967 - acc: 0.7501 - val_loss: 0.8627 - val_acc: 0.6850\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7806 - acc: 0.7520 - val_loss: 0.8538 - val_acc: 0.6910\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7653 - acc: 0.7564 - val_loss: 0.8380 - val_acc: 0.6980\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.7506 - acc: 0.7595 - val_loss: 0.8310 - val_acc: 0.7050\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7365 - acc: 0.7627 - val_loss: 0.8160 - val_acc: 0.7000\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.7238 - acc: 0.7644 - val_loss: 0.8061 - val_acc: 0.7110\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7110 - acc: 0.7677 - val_loss: 0.7986 - val_acc: 0.7060\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.7001 - acc: 0.7683 - val_loss: 0.7897 - val_acc: 0.7080\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6884 - acc: 0.7716 - val_loss: 0.7822 - val_acc: 0.7120\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6779 - acc: 0.7716 - val_loss: 0.7733 - val_acc: 0.7200\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 50us/step - loss: 0.6679 - acc: 0.7749 - val_loss: 0.7659 - val_acc: 0.7210\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.6587 - acc: 0.7776 - val_loss: 0.7596 - val_acc: 0.7170\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6496 - acc: 0.7773 - val_loss: 0.7544 - val_acc: 0.7240\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.6407 - acc: 0.7807 - val_loss: 0.7482 - val_acc: 0.7240\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.6328 - acc: 0.7827 - val_loss: 0.7427 - val_acc: 0.7240\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.6244 - acc: 0.7844 - val_loss: 0.7401 - val_acc: 0.7180\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6168 - acc: 0.7873 - val_loss: 0.7344 - val_acc: 0.7270\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.6099 - acc: 0.7907 - val_loss: 0.7285 - val_acc: 0.7290\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.6024 - acc: 0.7933 - val_loss: 0.7244 - val_acc: 0.7320\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.5959 - acc: 0.7940 - val_loss: 0.7215 - val_acc: 0.7290\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5892 - acc: 0.7959 - val_loss: 0.7183 - val_acc: 0.7260\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 0.5828 - acc: 0.7991 - val_loss: 0.7139 - val_acc: 0.7320\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"c78b93c8-549f-4208-a6d0-3627c7fa22c0\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"c78b93c8-549f-4208-a6d0-3627c7fa22c0\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n",
    "%notify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 42us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5770480327606201, 0.8001333333015442]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.672557727098465, 0.7459999996821086]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.5907 - acc: 0.1809 - val_loss: 2.5853 - val_acc: 0.1970\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.5686 - acc: 0.1969 - val_loss: 2.5646 - val_acc: 0.2020\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.5460 - acc: 0.2083 - val_loss: 2.5435 - val_acc: 0.2110\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.5225 - acc: 0.2143 - val_loss: 2.5212 - val_acc: 0.2150\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.4975 - acc: 0.2248 - val_loss: 2.4980 - val_acc: 0.2250\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.4709 - acc: 0.2349 - val_loss: 2.4724 - val_acc: 0.2370\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.4426 - acc: 0.2579 - val_loss: 2.4449 - val_acc: 0.2610\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.4121 - acc: 0.2765 - val_loss: 2.4144 - val_acc: 0.2850\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.3796 - acc: 0.3125 - val_loss: 2.3824 - val_acc: 0.3030\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.3451 - acc: 0.3455 - val_loss: 2.3478 - val_acc: 0.3310\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.3091 - acc: 0.3749 - val_loss: 2.3122 - val_acc: 0.3640\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.2710 - acc: 0.4061 - val_loss: 2.2745 - val_acc: 0.4090\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.2317 - acc: 0.4323 - val_loss: 2.2358 - val_acc: 0.4270\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.1910 - acc: 0.4588 - val_loss: 2.1963 - val_acc: 0.4570\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 2.1492 - acc: 0.4903 - val_loss: 2.1542 - val_acc: 0.4720\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.1067 - acc: 0.5127 - val_loss: 2.1126 - val_acc: 0.5100\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.0639 - acc: 0.5369 - val_loss: 2.0684 - val_acc: 0.5300\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.0205 - acc: 0.5635 - val_loss: 2.0260 - val_acc: 0.5460\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.9773 - acc: 0.5783 - val_loss: 1.9844 - val_acc: 0.5710\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9350 - acc: 0.5999 - val_loss: 1.9402 - val_acc: 0.5770\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8934 - acc: 0.6148 - val_loss: 1.9000 - val_acc: 0.6080\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8529 - acc: 0.6348 - val_loss: 1.8593 - val_acc: 0.6180\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8133 - acc: 0.6528 - val_loss: 1.8211 - val_acc: 0.6360\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7753 - acc: 0.6641 - val_loss: 1.7859 - val_acc: 0.6510\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7389 - acc: 0.6795 - val_loss: 1.7486 - val_acc: 0.6610\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7040 - acc: 0.6871 - val_loss: 1.7163 - val_acc: 0.6750\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.6706 - acc: 0.6964 - val_loss: 1.6846 - val_acc: 0.6810\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.6388 - acc: 0.7041 - val_loss: 1.6528 - val_acc: 0.6850\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.6088 - acc: 0.7087 - val_loss: 1.6256 - val_acc: 0.6940\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5801 - acc: 0.7192 - val_loss: 1.6036 - val_acc: 0.6890\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.5531 - acc: 0.7240 - val_loss: 1.5771 - val_acc: 0.6980\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.5276 - acc: 0.7264 - val_loss: 1.5526 - val_acc: 0.7010\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.5029 - acc: 0.7316 - val_loss: 1.5300 - val_acc: 0.7020\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4805 - acc: 0.7347 - val_loss: 1.5109 - val_acc: 0.7080\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4591 - acc: 0.7393 - val_loss: 1.4919 - val_acc: 0.7140\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.4388 - acc: 0.7443 - val_loss: 1.4736 - val_acc: 0.7110\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4197 - acc: 0.7477 - val_loss: 1.4576 - val_acc: 0.7190\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4016 - acc: 0.7496 - val_loss: 1.4418 - val_acc: 0.7180\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3847 - acc: 0.7525 - val_loss: 1.4297 - val_acc: 0.7190\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3682 - acc: 0.7592 - val_loss: 1.4133 - val_acc: 0.7200\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3528 - acc: 0.7611 - val_loss: 1.4019 - val_acc: 0.7250\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3377 - acc: 0.7619 - val_loss: 1.3876 - val_acc: 0.7250\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3239 - acc: 0.7649 - val_loss: 1.3787 - val_acc: 0.7220\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3107 - acc: 0.7688 - val_loss: 1.3725 - val_acc: 0.7250\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2977 - acc: 0.7679 - val_loss: 1.3563 - val_acc: 0.7290\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2853 - acc: 0.7716 - val_loss: 1.3463 - val_acc: 0.7260\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2730 - acc: 0.7741 - val_loss: 1.3406 - val_acc: 0.7290\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2621 - acc: 0.7768 - val_loss: 1.3281 - val_acc: 0.7320\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2515 - acc: 0.7795 - val_loss: 1.3208 - val_acc: 0.7290\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2407 - acc: 0.7811 - val_loss: 1.3137 - val_acc: 0.7340\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2308 - acc: 0.7827 - val_loss: 1.3069 - val_acc: 0.7370\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.2208 - acc: 0.7847 - val_loss: 1.2991 - val_acc: 0.7380\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2114 - acc: 0.7883 - val_loss: 1.2912 - val_acc: 0.7470\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2021 - acc: 0.7901 - val_loss: 1.2854 - val_acc: 0.7360\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1932 - acc: 0.7931 - val_loss: 1.2824 - val_acc: 0.7370\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1841 - acc: 0.7935 - val_loss: 1.2729 - val_acc: 0.7420\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1763 - acc: 0.8012 - val_loss: 1.2670 - val_acc: 0.7480\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1679 - acc: 0.7980 - val_loss: 1.2615 - val_acc: 0.7480\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1602 - acc: 0.8011 - val_loss: 1.2542 - val_acc: 0.7510\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1527 - acc: 0.8056 - val_loss: 1.2509 - val_acc: 0.7450\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1446 - acc: 0.8031 - val_loss: 1.2451 - val_acc: 0.7510\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1377 - acc: 0.8065 - val_loss: 1.2398 - val_acc: 0.7540\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1305 - acc: 0.8077 - val_loss: 1.2358 - val_acc: 0.7570\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1239 - acc: 0.8097 - val_loss: 1.2307 - val_acc: 0.7600\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1167 - acc: 0.8101 - val_loss: 1.2258 - val_acc: 0.7580\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1100 - acc: 0.8143 - val_loss: 1.2222 - val_acc: 0.7560\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1036 - acc: 0.8127 - val_loss: 1.2185 - val_acc: 0.7520\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0973 - acc: 0.8157 - val_loss: 1.2164 - val_acc: 0.7510\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0910 - acc: 0.8147 - val_loss: 1.2118 - val_acc: 0.7560\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0848 - acc: 0.8177 - val_loss: 1.2079 - val_acc: 0.7580\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0791 - acc: 0.8199 - val_loss: 1.2015 - val_acc: 0.7620\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0721 - acc: 0.8204 - val_loss: 1.1985 - val_acc: 0.7640\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0669 - acc: 0.8200 - val_loss: 1.1933 - val_acc: 0.7620\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0608 - acc: 0.8223 - val_loss: 1.1920 - val_acc: 0.7570\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0551 - acc: 0.8263 - val_loss: 1.1876 - val_acc: 0.7590\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0493 - acc: 0.8255 - val_loss: 1.1843 - val_acc: 0.7640\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0436 - acc: 0.8283 - val_loss: 1.1805 - val_acc: 0.7580\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0382 - acc: 0.8288 - val_loss: 1.1773 - val_acc: 0.7650\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0330 - acc: 0.8317 - val_loss: 1.1751 - val_acc: 0.7590\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0277 - acc: 0.8312 - val_loss: 1.1704 - val_acc: 0.7620\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0227 - acc: 0.8345 - val_loss: 1.1683 - val_acc: 0.7610\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0171 - acc: 0.8332 - val_loss: 1.1645 - val_acc: 0.7600\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0119 - acc: 0.8373 - val_loss: 1.1619 - val_acc: 0.7640\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0072 - acc: 0.8347 - val_loss: 1.1591 - val_acc: 0.7640\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0022 - acc: 0.8372 - val_loss: 1.1570 - val_acc: 0.7580\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9974 - acc: 0.8392 - val_loss: 1.1537 - val_acc: 0.7660\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9921 - acc: 0.8417 - val_loss: 1.1506 - val_acc: 0.7560\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9873 - acc: 0.8417 - val_loss: 1.1473 - val_acc: 0.7640\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9825 - acc: 0.8428 - val_loss: 1.1469 - val_acc: 0.7680\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9781 - acc: 0.8439 - val_loss: 1.1420 - val_acc: 0.7630\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9733 - acc: 0.8480 - val_loss: 1.1403 - val_acc: 0.7560\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9686 - acc: 0.8467 - val_loss: 1.1387 - val_acc: 0.7630\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9640 - acc: 0.8457 - val_loss: 1.1345 - val_acc: 0.7680\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9595 - acc: 0.8508 - val_loss: 1.1346 - val_acc: 0.7610\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9554 - acc: 0.8499 - val_loss: 1.1295 - val_acc: 0.7640\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9512 - acc: 0.8527 - val_loss: 1.1287 - val_acc: 0.7650\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9463 - acc: 0.8517 - val_loss: 1.1263 - val_acc: 0.7630\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9422 - acc: 0.8547 - val_loss: 1.1232 - val_acc: 0.7610\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9371 - acc: 0.8541 - val_loss: 1.1211 - val_acc: 0.7710\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9338 - acc: 0.8555 - val_loss: 1.1197 - val_acc: 0.7610\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9291 - acc: 0.8581 - val_loss: 1.1175 - val_acc: 0.7660\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9252 - acc: 0.8577 - val_loss: 1.1149 - val_acc: 0.7640\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9209 - acc: 0.8597 - val_loss: 1.1142 - val_acc: 0.7640\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9166 - acc: 0.8613 - val_loss: 1.1116 - val_acc: 0.7680\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9127 - acc: 0.8621 - val_loss: 1.1103 - val_acc: 0.7660\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9090 - acc: 0.8636 - val_loss: 1.1085 - val_acc: 0.7630\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9047 - acc: 0.8649 - val_loss: 1.1066 - val_acc: 0.7730\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9010 - acc: 0.8647 - val_loss: 1.1033 - val_acc: 0.7680\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8971 - acc: 0.8661 - val_loss: 1.1001 - val_acc: 0.7660\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8926 - acc: 0.8679 - val_loss: 1.0992 - val_acc: 0.7670\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8890 - acc: 0.8684 - val_loss: 1.0995 - val_acc: 0.7660\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8853 - acc: 0.8683 - val_loss: 1.0949 - val_acc: 0.7680\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8818 - acc: 0.8699 - val_loss: 1.0933 - val_acc: 0.7670\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8776 - acc: 0.8729 - val_loss: 1.0915 - val_acc: 0.7650\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8737 - acc: 0.8723 - val_loss: 1.0896 - val_acc: 0.7690\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8701 - acc: 0.8732 - val_loss: 1.0878 - val_acc: 0.7690\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8668 - acc: 0.8743 - val_loss: 1.0866 - val_acc: 0.7640\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8629 - acc: 0.8755 - val_loss: 1.0850 - val_acc: 0.7650\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8595 - acc: 0.8757 - val_loss: 1.0846 - val_acc: 0.7660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8562 - acc: 0.8771 - val_loss: 1.0819 - val_acc: 0.7650\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"f724d65e-9f91-46c8-9206-d7a1254920b8\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"f724d65e-9f91-46c8-9206-d7a1254920b8\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'g', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'r', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 16.0090 - acc: 0.1504 - val_loss: 15.6035 - val_acc: 0.1440\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 15.2375 - acc: 0.1735 - val_loss: 14.8571 - val_acc: 0.1680\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 14.5000 - acc: 0.1951 - val_loss: 14.1352 - val_acc: 0.2030\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 13.7862 - acc: 0.2288 - val_loss: 13.4346 - val_acc: 0.2180\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 13.0926 - acc: 0.2559 - val_loss: 12.7509 - val_acc: 0.2490\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 12.4176 - acc: 0.2956 - val_loss: 12.0866 - val_acc: 0.2860\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 11.7606 - acc: 0.3255 - val_loss: 11.4403 - val_acc: 0.3250\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 11.1215 - acc: 0.3541 - val_loss: 10.8118 - val_acc: 0.3540\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 10.5008 - acc: 0.3841 - val_loss: 10.2004 - val_acc: 0.3800\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 9.8989 - acc: 0.4041 - val_loss: 9.6093 - val_acc: 0.4110\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 9.3170 - acc: 0.4331 - val_loss: 9.0377 - val_acc: 0.4360\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 8.7569 - acc: 0.4587 - val_loss: 8.4890 - val_acc: 0.4570\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 8.2187 - acc: 0.4764 - val_loss: 7.9636 - val_acc: 0.5010\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 7.7043 - acc: 0.5072 - val_loss: 7.4607 - val_acc: 0.5180\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 7.2144 - acc: 0.5281 - val_loss: 6.9825 - val_acc: 0.5320\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 6.7472 - acc: 0.5520 - val_loss: 6.5274 - val_acc: 0.5450\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 6.3034 - acc: 0.5724 - val_loss: 6.0980 - val_acc: 0.5500\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 5.8832 - acc: 0.5812 - val_loss: 5.6912 - val_acc: 0.5860\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 5.4869 - acc: 0.6035 - val_loss: 5.3042 - val_acc: 0.5940\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 5.1138 - acc: 0.6139 - val_loss: 4.9437 - val_acc: 0.6180\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 4.7648 - acc: 0.6284 - val_loss: 4.6069 - val_acc: 0.6240\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 4.4386 - acc: 0.6379 - val_loss: 4.2926 - val_acc: 0.6280\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 4.1352 - acc: 0.6455 - val_loss: 3.9997 - val_acc: 0.6380\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 3.8547 - acc: 0.6500 - val_loss: 3.7345 - val_acc: 0.6390\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 3.5965 - acc: 0.6572 - val_loss: 3.4877 - val_acc: 0.6360\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 3.3603 - acc: 0.6609 - val_loss: 3.2603 - val_acc: 0.6440\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 3.1454 - acc: 0.6632 - val_loss: 3.0571 - val_acc: 0.6520\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.9525 - acc: 0.6691 - val_loss: 2.8744 - val_acc: 0.6530\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.7810 - acc: 0.6697 - val_loss: 2.7125 - val_acc: 0.6650\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.6303 - acc: 0.6744 - val_loss: 2.5722 - val_acc: 0.6650\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.5001 - acc: 0.6745 - val_loss: 2.4528 - val_acc: 0.6710\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 2.3896 - acc: 0.6775 - val_loss: 2.3507 - val_acc: 0.6680\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.2982 - acc: 0.6808 - val_loss: 2.2696 - val_acc: 0.6660\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 2.2251 - acc: 0.6809 - val_loss: 2.2028 - val_acc: 0.6700\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.1669 - acc: 0.6817 - val_loss: 2.1535 - val_acc: 0.6730\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 2.1244 - acc: 0.6816 - val_loss: 2.1159 - val_acc: 0.6760\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 2.0922 - acc: 0.6819 - val_loss: 2.0922 - val_acc: 0.6710\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 2.0667 - acc: 0.6819 - val_loss: 2.0659 - val_acc: 0.6780\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 64us/step - loss: 2.0445 - acc: 0.6825 - val_loss: 2.0433 - val_acc: 0.6700\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 2.0243 - acc: 0.6848 - val_loss: 2.0227 - val_acc: 0.6810\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.0055 - acc: 0.6835 - val_loss: 2.0045 - val_acc: 0.6800\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9880 - acc: 0.6864 - val_loss: 1.9904 - val_acc: 0.6740\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9717 - acc: 0.6852 - val_loss: 1.9734 - val_acc: 0.6740\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9555 - acc: 0.6840 - val_loss: 1.9591 - val_acc: 0.6790\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9405 - acc: 0.6864 - val_loss: 1.9413 - val_acc: 0.6830\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9251 - acc: 0.6873 - val_loss: 1.9292 - val_acc: 0.6850\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.9110 - acc: 0.6879 - val_loss: 1.9124 - val_acc: 0.6860\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8970 - acc: 0.6876 - val_loss: 1.9012 - val_acc: 0.6830\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.8840 - acc: 0.6883 - val_loss: 1.8849 - val_acc: 0.6850\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8708 - acc: 0.6889 - val_loss: 1.8722 - val_acc: 0.6880\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8585 - acc: 0.6895 - val_loss: 1.8616 - val_acc: 0.6870\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8461 - acc: 0.6909 - val_loss: 1.8499 - val_acc: 0.6830\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8346 - acc: 0.6907 - val_loss: 1.8377 - val_acc: 0.6880\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8229 - acc: 0.6909 - val_loss: 1.8255 - val_acc: 0.6880\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8117 - acc: 0.6931 - val_loss: 1.8137 - val_acc: 0.6860\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.8009 - acc: 0.6941 - val_loss: 1.8077 - val_acc: 0.6880\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7897 - acc: 0.6935 - val_loss: 1.7942 - val_acc: 0.6860\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7794 - acc: 0.6921 - val_loss: 1.7837 - val_acc: 0.6930\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7694 - acc: 0.6936 - val_loss: 1.7753 - val_acc: 0.6930\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7593 - acc: 0.6960 - val_loss: 1.7643 - val_acc: 0.6900\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7490 - acc: 0.6969 - val_loss: 1.7535 - val_acc: 0.6880\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7398 - acc: 0.6953 - val_loss: 1.7468 - val_acc: 0.6900\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7303 - acc: 0.6961 - val_loss: 1.7368 - val_acc: 0.6950\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7211 - acc: 0.6965 - val_loss: 1.7290 - val_acc: 0.6870\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7118 - acc: 0.6987 - val_loss: 1.7167 - val_acc: 0.6950\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7028 - acc: 0.6968 - val_loss: 1.7093 - val_acc: 0.6940\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6936 - acc: 0.6987 - val_loss: 1.6988 - val_acc: 0.6920\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.6855 - acc: 0.6992 - val_loss: 1.6894 - val_acc: 0.6920\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6766 - acc: 0.6996 - val_loss: 1.6827 - val_acc: 0.6960\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6683 - acc: 0.7005 - val_loss: 1.6754 - val_acc: 0.6910\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6601 - acc: 0.6989 - val_loss: 1.6667 - val_acc: 0.6950\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6521 - acc: 0.7012 - val_loss: 1.6569 - val_acc: 0.6960\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6440 - acc: 0.6999 - val_loss: 1.6497 - val_acc: 0.6950\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6358 - acc: 0.7019 - val_loss: 1.6450 - val_acc: 0.6970\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6281 - acc: 0.6999 - val_loss: 1.6352 - val_acc: 0.6980\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6204 - acc: 0.7019 - val_loss: 1.6267 - val_acc: 0.6990\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6130 - acc: 0.7028 - val_loss: 1.6197 - val_acc: 0.6960\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6051 - acc: 0.7020 - val_loss: 1.6159 - val_acc: 0.6970\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5980 - acc: 0.7039 - val_loss: 1.6079 - val_acc: 0.6990\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5912 - acc: 0.7047 - val_loss: 1.6005 - val_acc: 0.6990\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5837 - acc: 0.7044 - val_loss: 1.5941 - val_acc: 0.7000\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5773 - acc: 0.7055 - val_loss: 1.5871 - val_acc: 0.6950\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5708 - acc: 0.7043 - val_loss: 1.5807 - val_acc: 0.7020\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.5644 - acc: 0.7051 - val_loss: 1.5719 - val_acc: 0.6980\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.5576 - acc: 0.7047 - val_loss: 1.5687 - val_acc: 0.7050\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5511 - acc: 0.7080 - val_loss: 1.5626 - val_acc: 0.6990\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5446 - acc: 0.7048 - val_loss: 1.5543 - val_acc: 0.7020\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5381 - acc: 0.7069 - val_loss: 1.5530 - val_acc: 0.7000\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.5321 - acc: 0.7084 - val_loss: 1.5455 - val_acc: 0.7040\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5258 - acc: 0.7075 - val_loss: 1.5368 - val_acc: 0.7040\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5196 - acc: 0.7073 - val_loss: 1.5334 - val_acc: 0.7040\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.5136 - acc: 0.7101 - val_loss: 1.5249 - val_acc: 0.7070\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5077 - acc: 0.7089 - val_loss: 1.5272 - val_acc: 0.6990\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.5022 - acc: 0.7103 - val_loss: 1.5180 - val_acc: 0.7060\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4960 - acc: 0.7104 - val_loss: 1.5103 - val_acc: 0.7030\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4905 - acc: 0.7124 - val_loss: 1.5036 - val_acc: 0.7040\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4847 - acc: 0.7109 - val_loss: 1.4978 - val_acc: 0.7060\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.4795 - acc: 0.7128 - val_loss: 1.4919 - val_acc: 0.7100\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4737 - acc: 0.7116 - val_loss: 1.4861 - val_acc: 0.7060\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4681 - acc: 0.7123 - val_loss: 1.4869 - val_acc: 0.7080\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4629 - acc: 0.7136 - val_loss: 1.4760 - val_acc: 0.7100\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4576 - acc: 0.7144 - val_loss: 1.4716 - val_acc: 0.7100\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4520 - acc: 0.7144 - val_loss: 1.4685 - val_acc: 0.7090\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4464 - acc: 0.7157 - val_loss: 1.4630 - val_acc: 0.7060\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4416 - acc: 0.7137 - val_loss: 1.4543 - val_acc: 0.7120\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4365 - acc: 0.7157 - val_loss: 1.4513 - val_acc: 0.7120\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4313 - acc: 0.7159 - val_loss: 1.4457 - val_acc: 0.7110\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4262 - acc: 0.7168 - val_loss: 1.4452 - val_acc: 0.7090\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4213 - acc: 0.7172 - val_loss: 1.4378 - val_acc: 0.7140\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 49us/step - loss: 1.4166 - acc: 0.7175 - val_loss: 1.4321 - val_acc: 0.7090\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.4114 - acc: 0.7179 - val_loss: 1.4266 - val_acc: 0.7130\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4068 - acc: 0.7168 - val_loss: 1.4217 - val_acc: 0.7160\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.4020 - acc: 0.7181 - val_loss: 1.4170 - val_acc: 0.7130\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.3967 - acc: 0.7185 - val_loss: 1.4131 - val_acc: 0.7150\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3921 - acc: 0.7203 - val_loss: 1.4118 - val_acc: 0.7130\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3878 - acc: 0.7204 - val_loss: 1.4028 - val_acc: 0.7120\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3831 - acc: 0.7197 - val_loss: 1.4024 - val_acc: 0.7110\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.3789 - acc: 0.7213 - val_loss: 1.3992 - val_acc: 0.7090\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.3736 - acc: 0.7201 - val_loss: 1.3920 - val_acc: 0.7100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.3689 - acc: 0.7217 - val_loss: 1.3874 - val_acc: 0.7110\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"a6680488-a670-4382-aff0-18c7664b9b87\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"a6680488-a670-4382-aff0-18c7664b9b87\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 63us/step - loss: 16.0118 - acc: 0.1684 - val_loss: 15.6142 - val_acc: 0.1800\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 15.2387 - acc: 0.1908 - val_loss: 14.8572 - val_acc: 0.2110\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 14.4899 - acc: 0.2240 - val_loss: 14.1224 - val_acc: 0.2260\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 13.7629 - acc: 0.2556 - val_loss: 13.4086 - val_acc: 0.2470\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 13.0565 - acc: 0.2777 - val_loss: 12.7146 - val_acc: 0.2590\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 12.3701 - acc: 0.3040 - val_loss: 12.0398 - val_acc: 0.2850\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 11.7044 - acc: 0.3305 - val_loss: 11.3852 - val_acc: 0.3190\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 11.0599 - acc: 0.3645 - val_loss: 10.7532 - val_acc: 0.3430\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 10.4374 - acc: 0.3932 - val_loss: 10.1405 - val_acc: 0.3870\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 9.8363 - acc: 0.4235 - val_loss: 9.5490 - val_acc: 0.4400\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 9.2562 - acc: 0.4581 - val_loss: 8.9783 - val_acc: 0.4610\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 8.6972 - acc: 0.4877 - val_loss: 8.4306 - val_acc: 0.4810\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 8.1608 - acc: 0.5201 - val_loss: 7.9061 - val_acc: 0.5360\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 7.6471 - acc: 0.5516 - val_loss: 7.4043 - val_acc: 0.5540\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 7.1577 - acc: 0.5741 - val_loss: 6.9258 - val_acc: 0.5830\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 6.6920 - acc: 0.5908 - val_loss: 6.4727 - val_acc: 0.6020\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 6.2495 - acc: 0.6048 - val_loss: 6.0428 - val_acc: 0.6120\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 5.8308 - acc: 0.6219 - val_loss: 5.6347 - val_acc: 0.6170\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 5.4351 - acc: 0.6277 - val_loss: 5.2528 - val_acc: 0.6190\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 5.0634 - acc: 0.6344 - val_loss: 4.8919 - val_acc: 0.6380\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 4.7148 - acc: 0.6423 - val_loss: 4.5564 - val_acc: 0.6460\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 4.3889 - acc: 0.6459 - val_loss: 4.2404 - val_acc: 0.6520\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 4.0855 - acc: 0.6497 - val_loss: 3.9492 - val_acc: 0.6540\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 3.8053 - acc: 0.6509 - val_loss: 3.6793 - val_acc: 0.6620\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 3.5467 - acc: 0.6552 - val_loss: 3.4316 - val_acc: 0.6600\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 3.3099 - acc: 0.6583 - val_loss: 3.2071 - val_acc: 0.6630\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 3.0956 - acc: 0.6585 - val_loss: 3.0016 - val_acc: 0.6640\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 2.9028 - acc: 0.6596 - val_loss: 2.8200 - val_acc: 0.6740\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.7315 - acc: 0.6620 - val_loss: 2.6601 - val_acc: 0.6670\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.5812 - acc: 0.6615 - val_loss: 2.5191 - val_acc: 0.6730\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.4508 - acc: 0.6624 - val_loss: 2.3977 - val_acc: 0.6710\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.3395 - acc: 0.6625 - val_loss: 2.2952 - val_acc: 0.6680\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.2479 - acc: 0.6608 - val_loss: 2.2137 - val_acc: 0.6730\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 2.1739 - acc: 0.6617 - val_loss: 2.1486 - val_acc: 0.6700\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.1176 - acc: 0.6589 - val_loss: 2.1024 - val_acc: 0.6610\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 2.0751 - acc: 0.6611 - val_loss: 2.0635 - val_acc: 0.6710\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.0444 - acc: 0.6569 - val_loss: 2.0354 - val_acc: 0.6790\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 2.0195 - acc: 0.6609 - val_loss: 2.0152 - val_acc: 0.6630\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9982 - acc: 0.6575 - val_loss: 1.9929 - val_acc: 0.6710\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9794 - acc: 0.6579 - val_loss: 1.9746 - val_acc: 0.6830\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9618 - acc: 0.6600 - val_loss: 1.9562 - val_acc: 0.6780\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9456 - acc: 0.6608 - val_loss: 1.9397 - val_acc: 0.6730\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.9305 - acc: 0.6603 - val_loss: 1.9257 - val_acc: 0.6810\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.9163 - acc: 0.6597 - val_loss: 1.9133 - val_acc: 0.6770\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9021 - acc: 0.6608 - val_loss: 1.8979 - val_acc: 0.6760\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8886 - acc: 0.6604 - val_loss: 1.8847 - val_acc: 0.6790\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8757 - acc: 0.6617 - val_loss: 1.8724 - val_acc: 0.6780\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.8632 - acc: 0.6617 - val_loss: 1.8612 - val_acc: 0.6760\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8514 - acc: 0.6632 - val_loss: 1.8466 - val_acc: 0.6850\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8396 - acc: 0.6636 - val_loss: 1.8386 - val_acc: 0.6790\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.8285 - acc: 0.6649 - val_loss: 1.8247 - val_acc: 0.6670\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.8172 - acc: 0.6639 - val_loss: 1.8165 - val_acc: 0.6710\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8062 - acc: 0.6668 - val_loss: 1.8042 - val_acc: 0.6780\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7956 - acc: 0.6668 - val_loss: 1.7960 - val_acc: 0.6810\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7853 - acc: 0.6669 - val_loss: 1.7826 - val_acc: 0.6800\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7749 - acc: 0.6672 - val_loss: 1.7747 - val_acc: 0.6840\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.7659 - acc: 0.6683 - val_loss: 1.7623 - val_acc: 0.6770\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7550 - acc: 0.6685 - val_loss: 1.7517 - val_acc: 0.6840\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.7458 - acc: 0.6708 - val_loss: 1.7453 - val_acc: 0.6850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.7367 - acc: 0.6709 - val_loss: 1.7370 - val_acc: 0.6810\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7270 - acc: 0.6715 - val_loss: 1.7249 - val_acc: 0.6820\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7180 - acc: 0.6715 - val_loss: 1.7186 - val_acc: 0.6880\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7093 - acc: 0.6732 - val_loss: 1.7081 - val_acc: 0.6860\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.7004 - acc: 0.6743 - val_loss: 1.7002 - val_acc: 0.6910\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6923 - acc: 0.6747 - val_loss: 1.6907 - val_acc: 0.6860\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6832 - acc: 0.6760 - val_loss: 1.6835 - val_acc: 0.6880\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6749 - acc: 0.6768 - val_loss: 1.6811 - val_acc: 0.6870\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.6671 - acc: 0.676 - 0s 24us/step - loss: 1.6670 - acc: 0.6767 - val_loss: 1.6694 - val_acc: 0.6910\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6587 - acc: 0.6792 - val_loss: 1.6601 - val_acc: 0.6910\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6511 - acc: 0.6781 - val_loss: 1.6540 - val_acc: 0.6930\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6428 - acc: 0.6793 - val_loss: 1.6412 - val_acc: 0.6930\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6356 - acc: 0.6804 - val_loss: 1.6361 - val_acc: 0.6910\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6272 - acc: 0.6801 - val_loss: 1.6308 - val_acc: 0.6860\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6200 - acc: 0.6809 - val_loss: 1.6205 - val_acc: 0.6940\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.6127 - acc: 0.6807 - val_loss: 1.6160 - val_acc: 0.6900\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6054 - acc: 0.6813 - val_loss: 1.6090 - val_acc: 0.6950\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5977 - acc: 0.6827 - val_loss: 1.6002 - val_acc: 0.6960\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5907 - acc: 0.6843 - val_loss: 1.5921 - val_acc: 0.6960\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5834 - acc: 0.6839 - val_loss: 1.5851 - val_acc: 0.7010\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5765 - acc: 0.6849 - val_loss: 1.5798 - val_acc: 0.6970\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5691 - acc: 0.6841 - val_loss: 1.5720 - val_acc: 0.6990\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5625 - acc: 0.6855 - val_loss: 1.5675 - val_acc: 0.6970\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5561 - acc: 0.6873 - val_loss: 1.5563 - val_acc: 0.6980\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5485 - acc: 0.6859 - val_loss: 1.5542 - val_acc: 0.7010\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5420 - acc: 0.6867 - val_loss: 1.5448 - val_acc: 0.6950\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.5358 - acc: 0.6896 - val_loss: 1.5383 - val_acc: 0.6980\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5287 - acc: 0.6877 - val_loss: 1.5328 - val_acc: 0.7010\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5221 - acc: 0.6869 - val_loss: 1.5261 - val_acc: 0.6980\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.5158 - acc: 0.6903 - val_loss: 1.5195 - val_acc: 0.6970\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5096 - acc: 0.6905 - val_loss: 1.5135 - val_acc: 0.6960\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.5035 - acc: 0.6888 - val_loss: 1.5082 - val_acc: 0.6980\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4972 - acc: 0.6919 - val_loss: 1.5034 - val_acc: 0.7010\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4911 - acc: 0.6933 - val_loss: 1.4973 - val_acc: 0.6950\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4848 - acc: 0.6924 - val_loss: 1.4918 - val_acc: 0.7010\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4787 - acc: 0.6929 - val_loss: 1.4878 - val_acc: 0.6980\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4729 - acc: 0.6924 - val_loss: 1.4804 - val_acc: 0.6990\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4668 - acc: 0.6928 - val_loss: 1.4751 - val_acc: 0.6960\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4610 - acc: 0.6952 - val_loss: 1.4676 - val_acc: 0.7010\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4548 - acc: 0.6951 - val_loss: 1.4650 - val_acc: 0.7020\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4501 - acc: 0.6955 - val_loss: 1.4595 - val_acc: 0.7010\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4441 - acc: 0.6960 - val_loss: 1.4517 - val_acc: 0.6990\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4378 - acc: 0.6980 - val_loss: 1.4493 - val_acc: 0.7010\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4328 - acc: 0.6967 - val_loss: 1.4402 - val_acc: 0.7000\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4269 - acc: 0.6973 - val_loss: 1.4354 - val_acc: 0.7010\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4213 - acc: 0.6987 - val_loss: 1.4321 - val_acc: 0.7000\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4159 - acc: 0.6975 - val_loss: 1.4303 - val_acc: 0.7030\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.4109 - acc: 0.6996 - val_loss: 1.4231 - val_acc: 0.7010\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.4062 - acc: 0.6997 - val_loss: 1.4153 - val_acc: 0.7010\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.4001 - acc: 0.7013 - val_loss: 1.4156 - val_acc: 0.7020\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3956 - acc: 0.7016 - val_loss: 1.4074 - val_acc: 0.6990\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3907 - acc: 0.7020 - val_loss: 1.4018 - val_acc: 0.7010\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3851 - acc: 0.7035 - val_loss: 1.3975 - val_acc: 0.7020\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3800 - acc: 0.7021 - val_loss: 1.3963 - val_acc: 0.7000\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3757 - acc: 0.7025 - val_loss: 1.3873 - val_acc: 0.7020\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.3703 - acc: 0.7051 - val_loss: 1.3861 - val_acc: 0.7020\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3655 - acc: 0.7068 - val_loss: 1.3781 - val_acc: 0.7020\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3604 - acc: 0.7051 - val_loss: 1.3724 - val_acc: 0.7000\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.3576 - acc: 0.706 - 0s 27us/step - loss: 1.3551 - acc: 0.7056 - val_loss: 1.3711 - val_acc: 0.7030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3508 - acc: 0.7075 - val_loss: 1.3655 - val_acc: 0.7010\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3460 - acc: 0.7059 - val_loss: 1.3596 - val_acc: 0.7030\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3412 - acc: 0.7077 - val_loss: 1.3596 - val_acc: 0.7050\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3358 - acc: 0.7085 - val_loss: 1.3528 - val_acc: 0.7060\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3314 - acc: 0.7091 - val_loss: 1.3496 - val_acc: 0.7030\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3269 - acc: 0.7093 - val_loss: 1.3442 - val_acc: 0.7050\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3227 - acc: 0.7113 - val_loss: 1.3441 - val_acc: 0.7080\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3180 - acc: 0.7093 - val_loss: 1.3392 - val_acc: 0.7060\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3133 - acc: 0.7105 - val_loss: 1.3302 - val_acc: 0.7040\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3080 - acc: 0.7123 - val_loss: 1.3285 - val_acc: 0.7080\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3046 - acc: 0.7123 - val_loss: 1.3283 - val_acc: 0.7070\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2999 - acc: 0.7117 - val_loss: 1.3199 - val_acc: 0.7020\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.2932 - acc: 0.715 - 0s 43us/step - loss: 1.2959 - acc: 0.7124 - val_loss: 1.3171 - val_acc: 0.7030\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2917 - acc: 0.7120 - val_loss: 1.3138 - val_acc: 0.7060\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2875 - acc: 0.7143 - val_loss: 1.3074 - val_acc: 0.7040\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2828 - acc: 0.7145 - val_loss: 1.3021 - val_acc: 0.7060\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2786 - acc: 0.7161 - val_loss: 1.2998 - val_acc: 0.7080\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.2745 - acc: 0.7147 - val_loss: 1.3038 - val_acc: 0.7050\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.2710 - acc: 0.7152 - val_loss: 1.2941 - val_acc: 0.7060\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.2665 - acc: 0.7180 - val_loss: 1.2921 - val_acc: 0.7090\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.2627 - acc: 0.7153 - val_loss: 1.2876 - val_acc: 0.7080\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.2587 - acc: 0.7167 - val_loss: 1.2935 - val_acc: 0.7100\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2558 - acc: 0.7172 - val_loss: 1.2797 - val_acc: 0.7070\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2506 - acc: 0.7181 - val_loss: 1.2736 - val_acc: 0.7110\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2463 - acc: 0.7188 - val_loss: 1.2712 - val_acc: 0.7070\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2433 - acc: 0.7184 - val_loss: 1.2661 - val_acc: 0.7080\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2390 - acc: 0.7199 - val_loss: 1.2632 - val_acc: 0.7110\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.2353 - acc: 0.7191 - val_loss: 1.2606 - val_acc: 0.7090\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2322 - acc: 0.7195 - val_loss: 1.2589 - val_acc: 0.7100\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2280 - acc: 0.7200 - val_loss: 1.2571 - val_acc: 0.7080\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.2248 - acc: 0.7208 - val_loss: 1.2507 - val_acc: 0.7070\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.2210 - acc: 0.7207 - val_loss: 1.2470 - val_acc: 0.7130\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2169 - acc: 0.7212 - val_loss: 1.2440 - val_acc: 0.7130\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.2139 - acc: 0.7228 - val_loss: 1.2440 - val_acc: 0.7080\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.2106 - acc: 0.7213 - val_loss: 1.2381 - val_acc: 0.7140\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.2062 - acc: 0.7249 - val_loss: 1.2336 - val_acc: 0.7140\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.2028 - acc: 0.7243 - val_loss: 1.2344 - val_acc: 0.7110\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1999 - acc: 0.7233 - val_loss: 1.2308 - val_acc: 0.7120\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1965 - acc: 0.7256 - val_loss: 1.2264 - val_acc: 0.7110\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1924 - acc: 0.7257 - val_loss: 1.2284 - val_acc: 0.7120\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 47us/step - loss: 1.1904 - acc: 0.7253 - val_loss: 1.2201 - val_acc: 0.7160\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1864 - acc: 0.7265 - val_loss: 1.2188 - val_acc: 0.7140\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1836 - acc: 0.7255 - val_loss: 1.2148 - val_acc: 0.7190\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1801 - acc: 0.7271 - val_loss: 1.2152 - val_acc: 0.7160\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.1772 - acc: 0.7252 - val_loss: 1.2110 - val_acc: 0.7150\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.1745 - acc: 0.7292 - val_loss: 1.2103 - val_acc: 0.7180\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1718 - acc: 0.7271 - val_loss: 1.2053 - val_acc: 0.7160\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1689 - acc: 0.7272 - val_loss: 1.2024 - val_acc: 0.7190\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1658 - acc: 0.7272 - val_loss: 1.1992 - val_acc: 0.7140\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1634 - acc: 0.7271 - val_loss: 1.1960 - val_acc: 0.7190\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1605 - acc: 0.7281 - val_loss: 1.1993 - val_acc: 0.7190\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.1577 - acc: 0.7295 - val_loss: 1.1936 - val_acc: 0.7150\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1544 - acc: 0.7303 - val_loss: 1.1879 - val_acc: 0.7120\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1515 - acc: 0.7311 - val_loss: 1.1878 - val_acc: 0.7170\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1494 - acc: 0.7319 - val_loss: 1.1846 - val_acc: 0.7160\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.1471 - acc: 0.7320 - val_loss: 1.1830 - val_acc: 0.7150\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1442 - acc: 0.7327 - val_loss: 1.1845 - val_acc: 0.7200\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1419 - acc: 0.7307 - val_loss: 1.1791 - val_acc: 0.7160\n",
      "Epoch 177/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.1383 - acc: 0.7317 - val_loss: 1.1780 - val_acc: 0.7150\n",
      "Epoch 178/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1362 - acc: 0.7328 - val_loss: 1.1761 - val_acc: 0.7180\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 53us/step - loss: 1.1341 - acc: 0.7329 - val_loss: 1.1735 - val_acc: 0.7160\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 51us/step - loss: 1.1317 - acc: 0.7328 - val_loss: 1.1706 - val_acc: 0.7190\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 56us/step - loss: 1.1298 - acc: 0.7333 - val_loss: 1.1685 - val_acc: 0.7190\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 46us/step - loss: 1.1266 - acc: 0.7328 - val_loss: 1.1694 - val_acc: 0.7190\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1246 - acc: 0.7336 - val_loss: 1.1681 - val_acc: 0.7180\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 55us/step - loss: 1.1224 - acc: 0.7340 - val_loss: 1.1624 - val_acc: 0.7200\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 62us/step - loss: 1.1207 - acc: 0.7352 - val_loss: 1.1615 - val_acc: 0.7170\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.1177 - acc: 0.7348 - val_loss: 1.1599 - val_acc: 0.7210\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.1162 - acc: 0.7348 - val_loss: 1.1595 - val_acc: 0.7170\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1135 - acc: 0.7348 - val_loss: 1.1584 - val_acc: 0.7160\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1115 - acc: 0.7373 - val_loss: 1.1566 - val_acc: 0.7170\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1096 - acc: 0.7371 - val_loss: 1.1531 - val_acc: 0.7180\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1075 - acc: 0.7389 - val_loss: 1.1513 - val_acc: 0.7180\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1052 - acc: 0.7379 - val_loss: 1.1521 - val_acc: 0.7210\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1030 - acc: 0.7384 - val_loss: 1.1497 - val_acc: 0.7230\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.1016 - acc: 0.7383 - val_loss: 1.1452 - val_acc: 0.7230\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0994 - acc: 0.7383 - val_loss: 1.1447 - val_acc: 0.7230\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0978 - acc: 0.7387 - val_loss: 1.1414 - val_acc: 0.7200\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0950 - acc: 0.7388 - val_loss: 1.1466 - val_acc: 0.7180\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0937 - acc: 0.7389 - val_loss: 1.1415 - val_acc: 0.7180\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0924 - acc: 0.7396 - val_loss: 1.1401 - val_acc: 0.7150\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0897 - acc: 0.7405 - val_loss: 1.1395 - val_acc: 0.7180\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0875 - acc: 0.7395 - val_loss: 1.1404 - val_acc: 0.7190\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0857 - acc: 0.7401 - val_loss: 1.1336 - val_acc: 0.7240\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0835 - acc: 0.7407 - val_loss: 1.1305 - val_acc: 0.7290\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0821 - acc: 0.7416 - val_loss: 1.1338 - val_acc: 0.7190\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0806 - acc: 0.7424 - val_loss: 1.1338 - val_acc: 0.7230\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0785 - acc: 0.7424 - val_loss: 1.1323 - val_acc: 0.7210\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0768 - acc: 0.7423 - val_loss: 1.1258 - val_acc: 0.7240\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0747 - acc: 0.7448 - val_loss: 1.1270 - val_acc: 0.7220\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0731 - acc: 0.7424 - val_loss: 1.1246 - val_acc: 0.7250\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0717 - acc: 0.7429 - val_loss: 1.1240 - val_acc: 0.7200\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0691 - acc: 0.7425 - val_loss: 1.1206 - val_acc: 0.7280\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0676 - acc: 0.7448 - val_loss: 1.1182 - val_acc: 0.7260\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0660 - acc: 0.7443 - val_loss: 1.1184 - val_acc: 0.7280\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0645 - acc: 0.7439 - val_loss: 1.1177 - val_acc: 0.7230\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0623 - acc: 0.7449 - val_loss: 1.1184 - val_acc: 0.7180\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0605 - acc: 0.7457 - val_loss: 1.1171 - val_acc: 0.7200\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0592 - acc: 0.7463 - val_loss: 1.1132 - val_acc: 0.7230\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0583 - acc: 0.7444 - val_loss: 1.1129 - val_acc: 0.7220\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0562 - acc: 0.7468 - val_loss: 1.1149 - val_acc: 0.7270\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0549 - acc: 0.7471 - val_loss: 1.1122 - val_acc: 0.7240\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0534 - acc: 0.7483 - val_loss: 1.1072 - val_acc: 0.7290\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0515 - acc: 0.7468 - val_loss: 1.1080 - val_acc: 0.7240\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0503 - acc: 0.7459 - val_loss: 1.1061 - val_acc: 0.7240\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0490 - acc: 0.7477 - val_loss: 1.1084 - val_acc: 0.7260\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0479 - acc: 0.7485 - val_loss: 1.1043 - val_acc: 0.7260\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0459 - acc: 0.7492 - val_loss: 1.1035 - val_acc: 0.7200\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0447 - acc: 0.7479 - val_loss: 1.1036 - val_acc: 0.7250\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0430 - acc: 0.7481 - val_loss: 1.0985 - val_acc: 0.7300\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0413 - acc: 0.7488 - val_loss: 1.0984 - val_acc: 0.7260\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0403 - acc: 0.7479 - val_loss: 1.0974 - val_acc: 0.7300\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0386 - acc: 0.7495 - val_loss: 1.0988 - val_acc: 0.7240\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0370 - acc: 0.7499 - val_loss: 1.0941 - val_acc: 0.7290\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0362 - acc: 0.7507 - val_loss: 1.0934 - val_acc: 0.7280\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0345 - acc: 0.7512 - val_loss: 1.0992 - val_acc: 0.7240\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0335 - acc: 0.7501 - val_loss: 1.0920 - val_acc: 0.7250\n",
      "Epoch 236/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0316 - acc: 0.7501 - val_loss: 1.0919 - val_acc: 0.7250\n",
      "Epoch 237/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0305 - acc: 0.7501 - val_loss: 1.0974 - val_acc: 0.7260\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0294 - acc: 0.7523 - val_loss: 1.0875 - val_acc: 0.7290\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0275 - acc: 0.7504 - val_loss: 1.0906 - val_acc: 0.7260\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0273 - acc: 0.7500 - val_loss: 1.0868 - val_acc: 0.7250\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0260 - acc: 0.7496 - val_loss: 1.0855 - val_acc: 0.7230\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0243 - acc: 0.7521 - val_loss: 1.0867 - val_acc: 0.7290\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0235 - acc: 0.7503 - val_loss: 1.0859 - val_acc: 0.7270\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0222 - acc: 0.7523 - val_loss: 1.0860 - val_acc: 0.7290\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0208 - acc: 0.7515 - val_loss: 1.0872 - val_acc: 0.7270\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 1.0177 - acc: 0.750 - 0s 26us/step - loss: 1.0194 - acc: 0.7520 - val_loss: 1.0830 - val_acc: 0.7260\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0186 - acc: 0.7515 - val_loss: 1.0800 - val_acc: 0.7260\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0174 - acc: 0.7516 - val_loss: 1.0789 - val_acc: 0.7300\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0162 - acc: 0.7537 - val_loss: 1.0771 - val_acc: 0.7260\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0152 - acc: 0.7523 - val_loss: 1.0772 - val_acc: 0.7300\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0137 - acc: 0.7517 - val_loss: 1.0854 - val_acc: 0.7260\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0140 - acc: 0.7531 - val_loss: 1.0751 - val_acc: 0.7280\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0110 - acc: 0.7543 - val_loss: 1.0748 - val_acc: 0.7300\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0108 - acc: 0.7533 - val_loss: 1.0747 - val_acc: 0.7310\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0096 - acc: 0.7529 - val_loss: 1.0709 - val_acc: 0.7260\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0077 - acc: 0.7537 - val_loss: 1.0725 - val_acc: 0.7290\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0071 - acc: 0.7545 - val_loss: 1.0706 - val_acc: 0.7290\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0062 - acc: 0.7551 - val_loss: 1.0722 - val_acc: 0.7270\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0051 - acc: 0.7527 - val_loss: 1.0765 - val_acc: 0.7310\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0048 - acc: 0.7527 - val_loss: 1.0679 - val_acc: 0.7270\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0027 - acc: 0.7552 - val_loss: 1.0727 - val_acc: 0.7270\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0024 - acc: 0.7541 - val_loss: 1.0657 - val_acc: 0.7270\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0013 - acc: 0.7551 - val_loss: 1.0648 - val_acc: 0.7290\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9998 - acc: 0.7540 - val_loss: 1.0695 - val_acc: 0.7280\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9991 - acc: 0.7555 - val_loss: 1.0671 - val_acc: 0.7270\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9984 - acc: 0.7539 - val_loss: 1.0645 - val_acc: 0.7250\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9966 - acc: 0.7556 - val_loss: 1.0644 - val_acc: 0.7260\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9960 - acc: 0.7551 - val_loss: 1.0763 - val_acc: 0.7180\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9958 - acc: 0.7556 - val_loss: 1.0640 - val_acc: 0.7280\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9945 - acc: 0.7544 - val_loss: 1.0685 - val_acc: 0.7300\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9935 - acc: 0.7553 - val_loss: 1.0607 - val_acc: 0.7260\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9922 - acc: 0.7569 - val_loss: 1.0581 - val_acc: 0.7290\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9913 - acc: 0.7573 - val_loss: 1.0604 - val_acc: 0.7320\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9905 - acc: 0.7589 - val_loss: 1.0608 - val_acc: 0.7260\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9890 - acc: 0.7561 - val_loss: 1.0601 - val_acc: 0.7310\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9884 - acc: 0.7559 - val_loss: 1.0573 - val_acc: 0.7280\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9874 - acc: 0.7549 - val_loss: 1.0569 - val_acc: 0.7300\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9868 - acc: 0.7576 - val_loss: 1.0553 - val_acc: 0.7300\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9861 - acc: 0.7584 - val_loss: 1.0597 - val_acc: 0.7250\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9848 - acc: 0.7587 - val_loss: 1.0527 - val_acc: 0.7290\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9837 - acc: 0.7579 - val_loss: 1.0566 - val_acc: 0.7270\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9833 - acc: 0.7579 - val_loss: 1.0505 - val_acc: 0.7300\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9822 - acc: 0.7575 - val_loss: 1.0547 - val_acc: 0.7280\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9809 - acc: 0.7581 - val_loss: 1.0532 - val_acc: 0.7290\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9805 - acc: 0.7595 - val_loss: 1.0510 - val_acc: 0.7300\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9800 - acc: 0.7583 - val_loss: 1.0492 - val_acc: 0.7280\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9782 - acc: 0.7593 - val_loss: 1.0506 - val_acc: 0.7310\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9783 - acc: 0.7592 - val_loss: 1.0494 - val_acc: 0.7310\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9767 - acc: 0.7579 - val_loss: 1.0500 - val_acc: 0.7270\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9758 - acc: 0.7587 - val_loss: 1.0473 - val_acc: 0.7310\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9755 - acc: 0.7589 - val_loss: 1.0449 - val_acc: 0.7310\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9747 - acc: 0.7587 - val_loss: 1.0439 - val_acc: 0.7360\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9733 - acc: 0.7573 - val_loss: 1.0462 - val_acc: 0.7320\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9724 - acc: 0.7589 - val_loss: 1.0430 - val_acc: 0.7330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9712 - acc: 0.7589 - val_loss: 1.0435 - val_acc: 0.7270\n",
      "Epoch 296/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9713 - acc: 0.7593 - val_loss: 1.0524 - val_acc: 0.7260\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9715 - acc: 0.7588 - val_loss: 1.0433 - val_acc: 0.7320\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9699 - acc: 0.7593 - val_loss: 1.0407 - val_acc: 0.7310\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9690 - acc: 0.7576 - val_loss: 1.0445 - val_acc: 0.7280\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9684 - acc: 0.7615 - val_loss: 1.0410 - val_acc: 0.7300\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9668 - acc: 0.7596 - val_loss: 1.0403 - val_acc: 0.7290\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9657 - acc: 0.7596 - val_loss: 1.0395 - val_acc: 0.7340\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9656 - acc: 0.7609 - val_loss: 1.0375 - val_acc: 0.7300\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9645 - acc: 0.7609 - val_loss: 1.0448 - val_acc: 0.7270\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9647 - acc: 0.7624 - val_loss: 1.0368 - val_acc: 0.7320\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9629 - acc: 0.7615 - val_loss: 1.0401 - val_acc: 0.7290\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9617 - acc: 0.7592 - val_loss: 1.0398 - val_acc: 0.7280\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9614 - acc: 0.7611 - val_loss: 1.0412 - val_acc: 0.7240\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9605 - acc: 0.7627 - val_loss: 1.0353 - val_acc: 0.7300\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9607 - acc: 0.7609 - val_loss: 1.0338 - val_acc: 0.7300\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9596 - acc: 0.7620 - val_loss: 1.0362 - val_acc: 0.7310\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9591 - acc: 0.7600 - val_loss: 1.0363 - val_acc: 0.7330\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9580 - acc: 0.7600 - val_loss: 1.0321 - val_acc: 0.7350\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9576 - acc: 0.7595 - val_loss: 1.0345 - val_acc: 0.7280\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9566 - acc: 0.7613 - val_loss: 1.0331 - val_acc: 0.7340\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9559 - acc: 0.7623 - val_loss: 1.0302 - val_acc: 0.7330\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9555 - acc: 0.7612 - val_loss: 1.0357 - val_acc: 0.7340\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9556 - acc: 0.7612 - val_loss: 1.0379 - val_acc: 0.7300\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9546 - acc: 0.7605 - val_loss: 1.0306 - val_acc: 0.7280\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9538 - acc: 0.7617 - val_loss: 1.0311 - val_acc: 0.7350\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9528 - acc: 0.7612 - val_loss: 1.0305 - val_acc: 0.7300\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9527 - acc: 0.7621 - val_loss: 1.0293 - val_acc: 0.7340\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9517 - acc: 0.7615 - val_loss: 1.0301 - val_acc: 0.7300\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9512 - acc: 0.7633 - val_loss: 1.0280 - val_acc: 0.7340\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9498 - acc: 0.7632 - val_loss: 1.0313 - val_acc: 0.7280\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9497 - acc: 0.7645 - val_loss: 1.0265 - val_acc: 0.7330\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9492 - acc: 0.7620 - val_loss: 1.0294 - val_acc: 0.7320\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9494 - acc: 0.7605 - val_loss: 1.0277 - val_acc: 0.7380\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9481 - acc: 0.7641 - val_loss: 1.0263 - val_acc: 0.7340\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9476 - acc: 0.7632 - val_loss: 1.0243 - val_acc: 0.7310\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9461 - acc: 0.7639 - val_loss: 1.0246 - val_acc: 0.7360\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9473 - acc: 0.7637 - val_loss: 1.0273 - val_acc: 0.7270\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9457 - acc: 0.7644 - val_loss: 1.0241 - val_acc: 0.7310\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9447 - acc: 0.7645 - val_loss: 1.0238 - val_acc: 0.7320\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9451 - acc: 0.7637 - val_loss: 1.0219 - val_acc: 0.7310\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9435 - acc: 0.7643 - val_loss: 1.0219 - val_acc: 0.7340\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9431 - acc: 0.7640 - val_loss: 1.0224 - val_acc: 0.7320\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9424 - acc: 0.7644 - val_loss: 1.0220 - val_acc: 0.7320\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9424 - acc: 0.7643 - val_loss: 1.0207 - val_acc: 0.7370\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9420 - acc: 0.7629 - val_loss: 1.0240 - val_acc: 0.7320\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9413 - acc: 0.7653 - val_loss: 1.0241 - val_acc: 0.7360\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9406 - acc: 0.7641 - val_loss: 1.0205 - val_acc: 0.7360\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9396 - acc: 0.7625 - val_loss: 1.0180 - val_acc: 0.7350\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9388 - acc: 0.7663 - val_loss: 1.0178 - val_acc: 0.7330\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9399 - acc: 0.7633 - val_loss: 1.0203 - val_acc: 0.7350\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9383 - acc: 0.7656 - val_loss: 1.0176 - val_acc: 0.7340\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9385 - acc: 0.7657 - val_loss: 1.0230 - val_acc: 0.7320\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9369 - acc: 0.7648 - val_loss: 1.0183 - val_acc: 0.7360\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9366 - acc: 0.7657 - val_loss: 1.0174 - val_acc: 0.7360\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9366 - acc: 0.7651 - val_loss: 1.0308 - val_acc: 0.7300\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9375 - acc: 0.7657 - val_loss: 1.0190 - val_acc: 0.7320\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9350 - acc: 0.7653 - val_loss: 1.0169 - val_acc: 0.7360\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9350 - acc: 0.7656 - val_loss: 1.0169 - val_acc: 0.7300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9344 - acc: 0.7657 - val_loss: 1.0135 - val_acc: 0.7330\n",
      "Epoch 355/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9338 - acc: 0.7659 - val_loss: 1.0158 - val_acc: 0.7320\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9334 - acc: 0.7652 - val_loss: 1.0179 - val_acc: 0.7300\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9326 - acc: 0.7644 - val_loss: 1.0165 - val_acc: 0.7340\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9323 - acc: 0.7651 - val_loss: 1.0137 - val_acc: 0.7290\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9312 - acc: 0.7660 - val_loss: 1.0253 - val_acc: 0.7300\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9324 - acc: 0.7667 - val_loss: 1.0161 - val_acc: 0.7320\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9310 - acc: 0.7656 - val_loss: 1.0112 - val_acc: 0.7350\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9294 - acc: 0.7657 - val_loss: 1.0204 - val_acc: 0.7360\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9304 - acc: 0.7676 - val_loss: 1.0141 - val_acc: 0.7340\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9299 - acc: 0.7668 - val_loss: 1.0118 - val_acc: 0.7370\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9295 - acc: 0.7675 - val_loss: 1.0102 - val_acc: 0.7330\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9281 - acc: 0.7664 - val_loss: 1.0135 - val_acc: 0.7350\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9291 - acc: 0.7667 - val_loss: 1.0119 - val_acc: 0.7320\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9275 - acc: 0.7669 - val_loss: 1.0077 - val_acc: 0.7370\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9271 - acc: 0.7661 - val_loss: 1.0104 - val_acc: 0.7330\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9277 - acc: 0.7673 - val_loss: 1.0140 - val_acc: 0.7380\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9260 - acc: 0.7644 - val_loss: 1.0110 - val_acc: 0.7330\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9261 - acc: 0.7683 - val_loss: 1.0076 - val_acc: 0.7370\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9255 - acc: 0.7660 - val_loss: 1.0172 - val_acc: 0.7370\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9261 - acc: 0.7668 - val_loss: 1.0084 - val_acc: 0.7340\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9246 - acc: 0.7679 - val_loss: 1.0099 - val_acc: 0.7340\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9236 - acc: 0.7692 - val_loss: 1.0106 - val_acc: 0.7330\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9235 - acc: 0.7688 - val_loss: 1.0089 - val_acc: 0.7350\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9239 - acc: 0.7677 - val_loss: 1.0103 - val_acc: 0.7330\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9233 - acc: 0.7683 - val_loss: 1.0158 - val_acc: 0.7300\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9220 - acc: 0.7683 - val_loss: 1.0090 - val_acc: 0.7350\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9223 - acc: 0.7684 - val_loss: 1.0170 - val_acc: 0.7340\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9223 - acc: 0.7675 - val_loss: 1.0080 - val_acc: 0.7380\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9217 - acc: 0.7687 - val_loss: 1.0079 - val_acc: 0.7350\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9212 - acc: 0.7689 - val_loss: 1.0062 - val_acc: 0.7340\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9206 - acc: 0.7679 - val_loss: 1.0183 - val_acc: 0.7310\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9203 - acc: 0.7687 - val_loss: 1.0115 - val_acc: 0.7320\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9200 - acc: 0.7676 - val_loss: 1.0160 - val_acc: 0.7330\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9195 - acc: 0.7665 - val_loss: 1.0043 - val_acc: 0.7370\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9188 - acc: 0.7693 - val_loss: 1.0099 - val_acc: 0.7320\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9191 - acc: 0.7679 - val_loss: 1.0048 - val_acc: 0.7390\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9181 - acc: 0.7664 - val_loss: 1.0099 - val_acc: 0.7330\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9172 - acc: 0.7704 - val_loss: 1.0060 - val_acc: 0.7350\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9173 - acc: 0.7685 - val_loss: 1.0182 - val_acc: 0.7340\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9171 - acc: 0.7701 - val_loss: 1.0069 - val_acc: 0.7320\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9158 - acc: 0.7699 - val_loss: 1.0054 - val_acc: 0.7350\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9160 - acc: 0.7689 - val_loss: 1.0072 - val_acc: 0.7310\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9163 - acc: 0.7681 - val_loss: 1.0059 - val_acc: 0.7380\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9163 - acc: 0.7695 - val_loss: 1.0032 - val_acc: 0.7350\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9155 - acc: 0.7679 - val_loss: 1.0012 - val_acc: 0.7360\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9141 - acc: 0.7677 - val_loss: 1.0076 - val_acc: 0.7380\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9150 - acc: 0.7709 - val_loss: 1.0055 - val_acc: 0.7360\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9134 - acc: 0.7684 - val_loss: 1.0019 - val_acc: 0.7350\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9134 - acc: 0.7701 - val_loss: 1.0083 - val_acc: 0.7390\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9135 - acc: 0.7688 - val_loss: 1.0014 - val_acc: 0.7350\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9140 - acc: 0.7692 - val_loss: 1.0064 - val_acc: 0.7310\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9123 - acc: 0.7689 - val_loss: 0.9991 - val_acc: 0.7360\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9114 - acc: 0.7708 - val_loss: 1.0020 - val_acc: 0.7350\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9116 - acc: 0.7696 - val_loss: 1.0010 - val_acc: 0.7350\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9122 - acc: 0.7707 - val_loss: 1.0005 - val_acc: 0.7330\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9107 - acc: 0.7703 - val_loss: 1.0008 - val_acc: 0.7360\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9103 - acc: 0.7717 - val_loss: 1.0030 - val_acc: 0.7340\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9112 - acc: 0.7695 - val_loss: 1.0016 - val_acc: 0.7330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9100 - acc: 0.7708 - val_loss: 1.0009 - val_acc: 0.7330\n",
      "Epoch 414/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9098 - acc: 0.7701 - val_loss: 0.9984 - val_acc: 0.7340\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9096 - acc: 0.7697 - val_loss: 0.9984 - val_acc: 0.7330\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9099 - acc: 0.7720 - val_loss: 0.9989 - val_acc: 0.7350\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9084 - acc: 0.7704 - val_loss: 1.0028 - val_acc: 0.7330\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9085 - acc: 0.7717 - val_loss: 1.0125 - val_acc: 0.7330\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9092 - acc: 0.7704 - val_loss: 0.9984 - val_acc: 0.7340\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9073 - acc: 0.7695 - val_loss: 0.9985 - val_acc: 0.7350\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9074 - acc: 0.7696 - val_loss: 1.0038 - val_acc: 0.7340\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9067 - acc: 0.7731 - val_loss: 0.9990 - val_acc: 0.7340\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9064 - acc: 0.7715 - val_loss: 1.0033 - val_acc: 0.7350\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9064 - acc: 0.7715 - val_loss: 1.0006 - val_acc: 0.7350\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9057 - acc: 0.7724 - val_loss: 0.9962 - val_acc: 0.7350\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9051 - acc: 0.7728 - val_loss: 0.9990 - val_acc: 0.7380\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9044 - acc: 0.7724 - val_loss: 0.9956 - val_acc: 0.7360\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9043 - acc: 0.7727 - val_loss: 1.0000 - val_acc: 0.7350\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9049 - acc: 0.7737 - val_loss: 0.9975 - val_acc: 0.7380\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9031 - acc: 0.7744 - val_loss: 0.9957 - val_acc: 0.7370\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9043 - acc: 0.7716 - val_loss: 0.9970 - val_acc: 0.7370\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9032 - acc: 0.7721 - val_loss: 0.9972 - val_acc: 0.7320\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9035 - acc: 0.7732 - val_loss: 1.0065 - val_acc: 0.7390\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9025 - acc: 0.7728 - val_loss: 0.9960 - val_acc: 0.7340\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9013 - acc: 0.7724 - val_loss: 0.9936 - val_acc: 0.7350\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9020 - acc: 0.7712 - val_loss: 0.9950 - val_acc: 0.7380\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9013 - acc: 0.7708 - val_loss: 0.9999 - val_acc: 0.7330\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9013 - acc: 0.7728 - val_loss: 0.9974 - val_acc: 0.7340\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9011 - acc: 0.7721 - val_loss: 1.0014 - val_acc: 0.7290\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.9005 - acc: 0.7736 - val_loss: 1.0103 - val_acc: 0.7350\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9016 - acc: 0.7729 - val_loss: 0.9950 - val_acc: 0.7340\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9006 - acc: 0.7701 - val_loss: 0.9971 - val_acc: 0.7350\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8989 - acc: 0.7737 - val_loss: 0.9927 - val_acc: 0.7340\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8992 - acc: 0.7744 - val_loss: 0.9946 - val_acc: 0.7390\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8990 - acc: 0.7737 - val_loss: 0.9941 - val_acc: 0.7350\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8984 - acc: 0.7728 - val_loss: 0.9927 - val_acc: 0.7360\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8983 - acc: 0.7748 - val_loss: 0.9927 - val_acc: 0.7340\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8993 - acc: 0.7721 - val_loss: 0.9920 - val_acc: 0.7390\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8979 - acc: 0.7727 - val_loss: 0.9936 - val_acc: 0.7350\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8974 - acc: 0.7749 - val_loss: 0.9947 - val_acc: 0.7360\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8985 - acc: 0.7732 - val_loss: 0.9957 - val_acc: 0.7350\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8971 - acc: 0.7747 - val_loss: 0.9939 - val_acc: 0.7370\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8963 - acc: 0.7747 - val_loss: 1.0030 - val_acc: 0.7320\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8976 - acc: 0.7736 - val_loss: 1.0057 - val_acc: 0.7330\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8965 - acc: 0.7744 - val_loss: 1.0045 - val_acc: 0.7310\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8974 - acc: 0.7717 - val_loss: 0.9975 - val_acc: 0.7400\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8951 - acc: 0.7753 - val_loss: 0.9908 - val_acc: 0.7330\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8961 - acc: 0.7711 - val_loss: 0.9929 - val_acc: 0.7360\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8937 - acc: 0.7745 - val_loss: 0.9942 - val_acc: 0.7380\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8943 - acc: 0.7759 - val_loss: 0.9942 - val_acc: 0.7400\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8949 - acc: 0.7740 - val_loss: 0.9919 - val_acc: 0.7330\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8942 - acc: 0.7744 - val_loss: 0.9945 - val_acc: 0.7360\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8931 - acc: 0.7759 - val_loss: 0.9959 - val_acc: 0.7390\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8935 - acc: 0.7743 - val_loss: 0.9946 - val_acc: 0.7380\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8941 - acc: 0.7740 - val_loss: 1.0046 - val_acc: 0.7370\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8926 - acc: 0.7748 - val_loss: 0.9919 - val_acc: 0.7320\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8915 - acc: 0.7749 - val_loss: 0.9900 - val_acc: 0.7360\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8915 - acc: 0.7777 - val_loss: 0.9900 - val_acc: 0.7370\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8922 - acc: 0.7747 - val_loss: 0.9917 - val_acc: 0.7350\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8910 - acc: 0.7763 - val_loss: 0.9894 - val_acc: 0.7390\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8918 - acc: 0.7755 - val_loss: 0.9874 - val_acc: 0.7380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8904 - acc: 0.7747 - val_loss: 0.9885 - val_acc: 0.7360\n",
      "Epoch 473/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8914 - acc: 0.7736 - val_loss: 0.9882 - val_acc: 0.7350\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8910 - acc: 0.7759 - val_loss: 0.9905 - val_acc: 0.7340\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8907 - acc: 0.7775 - val_loss: 0.9900 - val_acc: 0.7350\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8901 - acc: 0.7757 - val_loss: 0.9926 - val_acc: 0.7360\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8896 - acc: 0.7768 - val_loss: 0.9867 - val_acc: 0.7350\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8892 - acc: 0.7759 - val_loss: 0.9920 - val_acc: 0.7340\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8891 - acc: 0.7781 - val_loss: 0.9890 - val_acc: 0.7360\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8892 - acc: 0.7764 - val_loss: 0.9944 - val_acc: 0.7380\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8874 - acc: 0.7763 - val_loss: 0.9909 - val_acc: 0.7280\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8891 - acc: 0.7757 - val_loss: 0.9869 - val_acc: 0.7350\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8872 - acc: 0.7760 - val_loss: 0.9884 - val_acc: 0.7360\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8885 - acc: 0.7756 - val_loss: 0.9895 - val_acc: 0.7400\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8882 - acc: 0.7752 - val_loss: 0.9869 - val_acc: 0.7350\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8881 - acc: 0.7745 - val_loss: 0.9863 - val_acc: 0.7380\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8866 - acc: 0.7749 - val_loss: 0.9933 - val_acc: 0.7400\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8873 - acc: 0.7787 - val_loss: 0.9884 - val_acc: 0.7360\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8868 - acc: 0.7757 - val_loss: 0.9922 - val_acc: 0.7320\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8865 - acc: 0.7764 - val_loss: 0.9845 - val_acc: 0.7360\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8865 - acc: 0.7769 - val_loss: 0.9848 - val_acc: 0.7370\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8853 - acc: 0.7785 - val_loss: 0.9882 - val_acc: 0.7330\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8865 - acc: 0.7761 - val_loss: 0.9899 - val_acc: 0.7320\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8851 - acc: 0.7760 - val_loss: 0.9873 - val_acc: 0.7350\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8842 - acc: 0.7767 - val_loss: 0.9960 - val_acc: 0.7350\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8846 - acc: 0.7775 - val_loss: 0.9900 - val_acc: 0.7390\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8848 - acc: 0.7761 - val_loss: 0.9870 - val_acc: 0.7350\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8838 - acc: 0.7763 - val_loss: 0.9842 - val_acc: 0.7380\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8838 - acc: 0.7775 - val_loss: 0.9947 - val_acc: 0.7290\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8853 - acc: 0.7757 - val_loss: 0.9852 - val_acc: 0.7350\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8843 - acc: 0.7776 - val_loss: 0.9862 - val_acc: 0.7350\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8836 - acc: 0.7788 - val_loss: 0.9901 - val_acc: 0.7330\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8826 - acc: 0.7776 - val_loss: 0.9905 - val_acc: 0.7330\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8823 - acc: 0.7769 - val_loss: 0.9870 - val_acc: 0.7340\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8823 - acc: 0.7785 - val_loss: 0.9861 - val_acc: 0.7350\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8840 - acc: 0.7761 - val_loss: 0.9893 - val_acc: 0.7370\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8810 - acc: 0.7788 - val_loss: 0.9858 - val_acc: 0.7340\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8825 - acc: 0.7789 - val_loss: 0.9862 - val_acc: 0.7310\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8816 - acc: 0.7791 - val_loss: 0.9889 - val_acc: 0.7320\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8801 - acc: 0.7775 - val_loss: 0.9833 - val_acc: 0.7370\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8802 - acc: 0.7788 - val_loss: 0.9836 - val_acc: 0.7390\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8806 - acc: 0.7771 - val_loss: 0.9833 - val_acc: 0.7370\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8804 - acc: 0.7753 - val_loss: 0.9864 - val_acc: 0.7350\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8803 - acc: 0.7780 - val_loss: 0.9827 - val_acc: 0.7360\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8804 - acc: 0.7759 - val_loss: 0.9844 - val_acc: 0.7360\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8796 - acc: 0.7768 - val_loss: 0.9912 - val_acc: 0.7340\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8783 - acc: 0.7789 - val_loss: 0.9852 - val_acc: 0.7400\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8788 - acc: 0.7783 - val_loss: 0.9868 - val_acc: 0.7350\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8786 - acc: 0.7792 - val_loss: 0.9811 - val_acc: 0.7350\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8781 - acc: 0.7785 - val_loss: 0.9870 - val_acc: 0.7330\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8784 - acc: 0.7765 - val_loss: 0.9814 - val_acc: 0.7370\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8777 - acc: 0.7795 - val_loss: 0.9893 - val_acc: 0.7390\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8777 - acc: 0.7775 - val_loss: 0.9930 - val_acc: 0.7360\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8801 - acc: 0.7763 - val_loss: 0.9807 - val_acc: 0.7350\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8774 - acc: 0.7795 - val_loss: 1.0021 - val_acc: 0.7380\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8787 - acc: 0.7787 - val_loss: 0.9895 - val_acc: 0.7290\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8764 - acc: 0.7803 - val_loss: 0.9838 - val_acc: 0.7350\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8762 - acc: 0.7791 - val_loss: 0.9805 - val_acc: 0.7330\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8764 - acc: 0.7801 - val_loss: 0.9864 - val_acc: 0.7390\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8771 - acc: 0.7803 - val_loss: 0.9926 - val_acc: 0.7340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8759 - acc: 0.7809 - val_loss: 1.0015 - val_acc: 0.7380\n",
      "Epoch 532/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8755 - acc: 0.7808 - val_loss: 0.9806 - val_acc: 0.7350\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8750 - acc: 0.7807 - val_loss: 0.9938 - val_acc: 0.7340\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8751 - acc: 0.7800 - val_loss: 0.9903 - val_acc: 0.7380\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8760 - acc: 0.7807 - val_loss: 0.9909 - val_acc: 0.7350\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8749 - acc: 0.7812 - val_loss: 0.9826 - val_acc: 0.7370\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8734 - acc: 0.7816 - val_loss: 0.9858 - val_acc: 0.7350\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8748 - acc: 0.7797 - val_loss: 0.9789 - val_acc: 0.7340\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8744 - acc: 0.7799 - val_loss: 0.9805 - val_acc: 0.7360\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8742 - acc: 0.7795 - val_loss: 0.9828 - val_acc: 0.7410\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8737 - acc: 0.7803 - val_loss: 0.9831 - val_acc: 0.7380\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8738 - acc: 0.7796 - val_loss: 0.9867 - val_acc: 0.7350\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8733 - acc: 0.7807 - val_loss: 0.9792 - val_acc: 0.7360\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8739 - acc: 0.7809 - val_loss: 0.9778 - val_acc: 0.7340\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8720 - acc: 0.7819 - val_loss: 0.9823 - val_acc: 0.7310\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8727 - acc: 0.7807 - val_loss: 0.9826 - val_acc: 0.7350\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8724 - acc: 0.7797 - val_loss: 0.9792 - val_acc: 0.7390\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8723 - acc: 0.7800 - val_loss: 0.9802 - val_acc: 0.7370\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8725 - acc: 0.7808 - val_loss: 0.9840 - val_acc: 0.7300\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8712 - acc: 0.7816 - val_loss: 0.9832 - val_acc: 0.7340\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8722 - acc: 0.7815 - val_loss: 0.9896 - val_acc: 0.7380\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8705 - acc: 0.7815 - val_loss: 0.9842 - val_acc: 0.7350\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8705 - acc: 0.7820 - val_loss: 0.9781 - val_acc: 0.7370\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8709 - acc: 0.7824 - val_loss: 0.9851 - val_acc: 0.7360\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8712 - acc: 0.7797 - val_loss: 0.9922 - val_acc: 0.7370\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8717 - acc: 0.7804 - val_loss: 0.9817 - val_acc: 0.7410\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8704 - acc: 0.7827 - val_loss: 0.9799 - val_acc: 0.7380\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8692 - acc: 0.7816 - val_loss: 0.9860 - val_acc: 0.7310\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8702 - acc: 0.7801 - val_loss: 0.9787 - val_acc: 0.7360\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8693 - acc: 0.7833 - val_loss: 0.9813 - val_acc: 0.7370\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8697 - acc: 0.7801 - val_loss: 0.9781 - val_acc: 0.7380\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8685 - acc: 0.7817 - val_loss: 0.9796 - val_acc: 0.7390\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8677 - acc: 0.7836 - val_loss: 0.9906 - val_acc: 0.7330\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8684 - acc: 0.7801 - val_loss: 0.9810 - val_acc: 0.7400\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8693 - acc: 0.7813 - val_loss: 0.9816 - val_acc: 0.7340\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8668 - acc: 0.7820 - val_loss: 0.9752 - val_acc: 0.7360\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8669 - acc: 0.7805 - val_loss: 0.9757 - val_acc: 0.7360\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8674 - acc: 0.7823 - val_loss: 0.9774 - val_acc: 0.7380\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8675 - acc: 0.7820 - val_loss: 0.9882 - val_acc: 0.7360\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8677 - acc: 0.7819 - val_loss: 0.9927 - val_acc: 0.7370\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8680 - acc: 0.7787 - val_loss: 0.9823 - val_acc: 0.7400\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8667 - acc: 0.7845 - val_loss: 0.9915 - val_acc: 0.7290\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8666 - acc: 0.7840 - val_loss: 0.9842 - val_acc: 0.7390\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8658 - acc: 0.7824 - val_loss: 1.0021 - val_acc: 0.7270\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8659 - acc: 0.7845 - val_loss: 0.9950 - val_acc: 0.7400\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8666 - acc: 0.7821 - val_loss: 0.9799 - val_acc: 0.7400\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8652 - acc: 0.7825 - val_loss: 0.9826 - val_acc: 0.7380\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8653 - acc: 0.7816 - val_loss: 0.9805 - val_acc: 0.7380\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8646 - acc: 0.7823 - val_loss: 0.9745 - val_acc: 0.7360\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8655 - acc: 0.7797 - val_loss: 0.9850 - val_acc: 0.7300\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8669 - acc: 0.7831 - val_loss: 0.9848 - val_acc: 0.7360\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8640 - acc: 0.7833 - val_loss: 0.9745 - val_acc: 0.7390\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8630 - acc: 0.7829 - val_loss: 0.9942 - val_acc: 0.7400\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8648 - acc: 0.7829 - val_loss: 0.9804 - val_acc: 0.7340\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8643 - acc: 0.7827 - val_loss: 0.9948 - val_acc: 0.7320\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8650 - acc: 0.7815 - val_loss: 0.9772 - val_acc: 0.7400\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8638 - acc: 0.7829 - val_loss: 0.9803 - val_acc: 0.7370\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8646 - acc: 0.7839 - val_loss: 0.9751 - val_acc: 0.7350\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8640 - acc: 0.7832 - val_loss: 0.9742 - val_acc: 0.7360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8626 - acc: 0.7847 - val_loss: 0.9782 - val_acc: 0.7390\n",
      "Epoch 591/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8626 - acc: 0.7844 - val_loss: 0.9882 - val_acc: 0.7310\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8621 - acc: 0.7855 - val_loss: 0.9845 - val_acc: 0.7340\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8629 - acc: 0.7824 - val_loss: 0.9837 - val_acc: 0.7390\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8610 - acc: 0.7835 - val_loss: 0.9766 - val_acc: 0.7420\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8614 - acc: 0.7837 - val_loss: 0.9843 - val_acc: 0.7350\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8621 - acc: 0.7853 - val_loss: 0.9754 - val_acc: 0.7360\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8609 - acc: 0.7827 - val_loss: 0.9762 - val_acc: 0.7380\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8599 - acc: 0.7829 - val_loss: 0.9729 - val_acc: 0.7360\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8597 - acc: 0.7859 - val_loss: 0.9755 - val_acc: 0.7370\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8604 - acc: 0.7845 - val_loss: 0.9832 - val_acc: 0.7410\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8603 - acc: 0.7859 - val_loss: 0.9836 - val_acc: 0.7370\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8609 - acc: 0.7847 - val_loss: 0.9868 - val_acc: 0.7380\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8617 - acc: 0.7825 - val_loss: 0.9737 - val_acc: 0.7380\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8594 - acc: 0.7840 - val_loss: 0.9792 - val_acc: 0.7310\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8581 - acc: 0.7851 - val_loss: 0.9823 - val_acc: 0.7350\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8583 - acc: 0.7859 - val_loss: 0.9739 - val_acc: 0.7400\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8594 - acc: 0.7844 - val_loss: 0.9747 - val_acc: 0.7410\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8594 - acc: 0.7836 - val_loss: 0.9786 - val_acc: 0.7410\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8589 - acc: 0.7821 - val_loss: 0.9817 - val_acc: 0.7310\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8580 - acc: 0.7851 - val_loss: 0.9769 - val_acc: 0.7360\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8586 - acc: 0.7833 - val_loss: 0.9754 - val_acc: 0.7350\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8588 - acc: 0.7860 - val_loss: 0.9750 - val_acc: 0.7390\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8582 - acc: 0.7829 - val_loss: 0.9778 - val_acc: 0.7360\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8596 - acc: 0.7841 - val_loss: 0.9744 - val_acc: 0.7410\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8569 - acc: 0.7833 - val_loss: 0.9707 - val_acc: 0.7400\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8565 - acc: 0.7856 - val_loss: 0.9731 - val_acc: 0.7370\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8592 - acc: 0.7863 - val_loss: 0.9829 - val_acc: 0.7370\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8586 - acc: 0.7851 - val_loss: 0.9709 - val_acc: 0.7400\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8558 - acc: 0.7857 - val_loss: 0.9717 - val_acc: 0.7380\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8565 - acc: 0.7867 - val_loss: 0.9836 - val_acc: 0.7350\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8567 - acc: 0.7869 - val_loss: 0.9754 - val_acc: 0.7360\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8589 - acc: 0.7829 - val_loss: 0.9880 - val_acc: 0.7380\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8558 - acc: 0.7853 - val_loss: 0.9719 - val_acc: 0.7380\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8553 - acc: 0.7851 - val_loss: 0.9846 - val_acc: 0.7320\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8559 - acc: 0.7840 - val_loss: 0.9724 - val_acc: 0.7410\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8545 - acc: 0.7863 - val_loss: 0.9712 - val_acc: 0.7410\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8545 - acc: 0.7849 - val_loss: 0.9771 - val_acc: 0.7370\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8565 - acc: 0.7855 - val_loss: 0.9751 - val_acc: 0.7380\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8540 - acc: 0.7871 - val_loss: 0.9753 - val_acc: 0.7420\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8541 - acc: 0.7860 - val_loss: 0.9710 - val_acc: 0.7420\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8541 - acc: 0.7851 - val_loss: 0.9995 - val_acc: 0.7270\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8549 - acc: 0.7848 - val_loss: 0.9738 - val_acc: 0.7380\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8534 - acc: 0.7841 - val_loss: 0.9770 - val_acc: 0.7360\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8541 - acc: 0.7860 - val_loss: 0.9725 - val_acc: 0.7370\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8544 - acc: 0.7847 - val_loss: 0.9715 - val_acc: 0.7400\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8537 - acc: 0.7859 - val_loss: 0.9705 - val_acc: 0.7410\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8537 - acc: 0.7857 - val_loss: 0.9764 - val_acc: 0.7340\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8539 - acc: 0.7864 - val_loss: 0.9691 - val_acc: 0.7400\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8522 - acc: 0.7859 - val_loss: 0.9756 - val_acc: 0.7380\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8520 - acc: 0.7860 - val_loss: 0.9792 - val_acc: 0.7400\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8523 - acc: 0.7868 - val_loss: 0.9877 - val_acc: 0.7300\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8531 - acc: 0.7880 - val_loss: 0.9719 - val_acc: 0.7400\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8520 - acc: 0.7856 - val_loss: 0.9788 - val_acc: 0.7300\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8529 - acc: 0.7845 - val_loss: 0.9738 - val_acc: 0.7380\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8512 - acc: 0.7840 - val_loss: 0.9734 - val_acc: 0.7360\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8517 - acc: 0.7872 - val_loss: 0.9791 - val_acc: 0.7410\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8514 - acc: 0.7851 - val_loss: 0.9731 - val_acc: 0.7340\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8511 - acc: 0.7868 - val_loss: 0.9720 - val_acc: 0.7330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8510 - acc: 0.7864 - val_loss: 0.9762 - val_acc: 0.7330\n",
      "Epoch 650/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8510 - acc: 0.7860 - val_loss: 0.9823 - val_acc: 0.7350\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8519 - acc: 0.7869 - val_loss: 0.9737 - val_acc: 0.7420\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8497 - acc: 0.7868 - val_loss: 0.9707 - val_acc: 0.7400\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8501 - acc: 0.7853 - val_loss: 0.9715 - val_acc: 0.7390\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8495 - acc: 0.7871 - val_loss: 0.9801 - val_acc: 0.7410\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8490 - acc: 0.7853 - val_loss: 0.9704 - val_acc: 0.7390\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8500 - acc: 0.7863 - val_loss: 0.9684 - val_acc: 0.7390\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8489 - acc: 0.7881 - val_loss: 0.9724 - val_acc: 0.7390\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8487 - acc: 0.7868 - val_loss: 0.9793 - val_acc: 0.7350\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8495 - acc: 0.7869 - val_loss: 0.9742 - val_acc: 0.7400\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8487 - acc: 0.7867 - val_loss: 0.9779 - val_acc: 0.7370\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8489 - acc: 0.7841 - val_loss: 0.9816 - val_acc: 0.7350\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8491 - acc: 0.7871 - val_loss: 0.9704 - val_acc: 0.7380\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8499 - acc: 0.7869 - val_loss: 0.9734 - val_acc: 0.7320\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8481 - acc: 0.7879 - val_loss: 0.9717 - val_acc: 0.7360\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8496 - acc: 0.7861 - val_loss: 0.9697 - val_acc: 0.7380\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8477 - acc: 0.7871 - val_loss: 0.9670 - val_acc: 0.7400\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8467 - acc: 0.7875 - val_loss: 0.9697 - val_acc: 0.7400\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8475 - acc: 0.7868 - val_loss: 0.9676 - val_acc: 0.7370\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8467 - acc: 0.7867 - val_loss: 0.9754 - val_acc: 0.7340\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8465 - acc: 0.7884 - val_loss: 0.9743 - val_acc: 0.7340\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8479 - acc: 0.7891 - val_loss: 0.9690 - val_acc: 0.7390\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8458 - acc: 0.7871 - val_loss: 0.9900 - val_acc: 0.7310\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8471 - acc: 0.7868 - val_loss: 0.9723 - val_acc: 0.7440\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8448 - acc: 0.7881 - val_loss: 0.9786 - val_acc: 0.7360\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8470 - acc: 0.7871 - val_loss: 0.9773 - val_acc: 0.7430\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8460 - acc: 0.7883 - val_loss: 0.9691 - val_acc: 0.7370\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8477 - acc: 0.7867 - val_loss: 0.9728 - val_acc: 0.7380\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8453 - acc: 0.7875 - val_loss: 0.9886 - val_acc: 0.7400\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8480 - acc: 0.7855 - val_loss: 0.9735 - val_acc: 0.7340\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8446 - acc: 0.7884 - val_loss: 0.9714 - val_acc: 0.7390\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8451 - acc: 0.7891 - val_loss: 0.9876 - val_acc: 0.7340\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8452 - acc: 0.7865 - val_loss: 0.9755 - val_acc: 0.7370\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8458 - acc: 0.7883 - val_loss: 0.9719 - val_acc: 0.7380\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8447 - acc: 0.7888 - val_loss: 0.9684 - val_acc: 0.7410\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8435 - acc: 0.7892 - val_loss: 0.9899 - val_acc: 0.7300\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8450 - acc: 0.7887 - val_loss: 0.9659 - val_acc: 0.7360\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8452 - acc: 0.7868 - val_loss: 0.9823 - val_acc: 0.7310\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8438 - acc: 0.7875 - val_loss: 0.9830 - val_acc: 0.7390\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8436 - acc: 0.7876 - val_loss: 0.9724 - val_acc: 0.7400\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8420 - acc: 0.7889 - val_loss: 0.9829 - val_acc: 0.7380\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8440 - acc: 0.7849 - val_loss: 0.9666 - val_acc: 0.7410\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8433 - acc: 0.7883 - val_loss: 0.9677 - val_acc: 0.7360\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8423 - acc: 0.7883 - val_loss: 0.9714 - val_acc: 0.7360\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8426 - acc: 0.7880 - val_loss: 0.9750 - val_acc: 0.7320\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8432 - acc: 0.7915 - val_loss: 0.9722 - val_acc: 0.7360\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8424 - acc: 0.7908 - val_loss: 0.9675 - val_acc: 0.7370\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8419 - acc: 0.7891 - val_loss: 0.9742 - val_acc: 0.7390\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8412 - acc: 0.7909 - val_loss: 0.9681 - val_acc: 0.7420\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8434 - acc: 0.7892 - val_loss: 0.9712 - val_acc: 0.7370\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8411 - acc: 0.7899 - val_loss: 0.9696 - val_acc: 0.7440\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8412 - acc: 0.7901 - val_loss: 0.9668 - val_acc: 0.7400\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8414 - acc: 0.7907 - val_loss: 0.9736 - val_acc: 0.7440\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8412 - acc: 0.7875 - val_loss: 0.9850 - val_acc: 0.7360\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8426 - acc: 0.7884 - val_loss: 0.9754 - val_acc: 0.7400\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8408 - acc: 0.7899 - val_loss: 0.9736 - val_acc: 0.7350\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8405 - acc: 0.7897 - val_loss: 0.9658 - val_acc: 0.7420\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8409 - acc: 0.7895 - val_loss: 0.9642 - val_acc: 0.7350\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8405 - acc: 0.7904 - val_loss: 0.9645 - val_acc: 0.7410\n",
      "Epoch 709/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8377 - acc: 0.7904 - val_loss: 0.9772 - val_acc: 0.7410\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8422 - acc: 0.7899 - val_loss: 0.9689 - val_acc: 0.7410\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8395 - acc: 0.7888 - val_loss: 0.9780 - val_acc: 0.7410\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8401 - acc: 0.7888 - val_loss: 0.9744 - val_acc: 0.7380\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8384 - acc: 0.7889 - val_loss: 0.9775 - val_acc: 0.7420\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8416 - acc: 0.7901 - val_loss: 0.9642 - val_acc: 0.7380\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8380 - acc: 0.7908 - val_loss: 0.9665 - val_acc: 0.7430\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8383 - acc: 0.7903 - val_loss: 0.9687 - val_acc: 0.7420\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8408 - acc: 0.7896 - val_loss: 0.9735 - val_acc: 0.7410\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8393 - acc: 0.7873 - val_loss: 0.9642 - val_acc: 0.7400\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8376 - acc: 0.7917 - val_loss: 0.9650 - val_acc: 0.7420\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8375 - acc: 0.7904 - val_loss: 0.9692 - val_acc: 0.7410\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8379 - acc: 0.7909 - val_loss: 0.9725 - val_acc: 0.7330\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8393 - acc: 0.7907 - val_loss: 0.9664 - val_acc: 0.7440\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8382 - acc: 0.7888 - val_loss: 0.9708 - val_acc: 0.7350\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8389 - acc: 0.7919 - val_loss: 0.9725 - val_acc: 0.7330\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8379 - acc: 0.7880 - val_loss: 0.9748 - val_acc: 0.7330\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8374 - acc: 0.7917 - val_loss: 0.9715 - val_acc: 0.7410\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8384 - acc: 0.7904 - val_loss: 0.9671 - val_acc: 0.7420\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8364 - acc: 0.7907 - val_loss: 0.9675 - val_acc: 0.7400\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8377 - acc: 0.7919 - val_loss: 0.9714 - val_acc: 0.7390\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8360 - acc: 0.7920 - val_loss: 0.9640 - val_acc: 0.7360\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8366 - acc: 0.7913 - val_loss: 0.9682 - val_acc: 0.7380\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8360 - acc: 0.7909 - val_loss: 0.9678 - val_acc: 0.7410\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8347 - acc: 0.7923 - val_loss: 0.9646 - val_acc: 0.7390\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8359 - acc: 0.7903 - val_loss: 0.9651 - val_acc: 0.7430\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8366 - acc: 0.7907 - val_loss: 0.9671 - val_acc: 0.7460\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8341 - acc: 0.7900 - val_loss: 0.9668 - val_acc: 0.7420\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8352 - acc: 0.7929 - val_loss: 0.9832 - val_acc: 0.7390\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8354 - acc: 0.7919 - val_loss: 0.9620 - val_acc: 0.7390\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8343 - acc: 0.7904 - val_loss: 0.9656 - val_acc: 0.7410\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8344 - acc: 0.7933 - val_loss: 0.9692 - val_acc: 0.7420\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8348 - acc: 0.7903 - val_loss: 0.9679 - val_acc: 0.7450\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8354 - acc: 0.7903 - val_loss: 0.9674 - val_acc: 0.7420\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8353 - acc: 0.7915 - val_loss: 0.9764 - val_acc: 0.7360\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8343 - acc: 0.7899 - val_loss: 0.9617 - val_acc: 0.7400\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8334 - acc: 0.7915 - val_loss: 0.9694 - val_acc: 0.7380\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8332 - acc: 0.7948 - val_loss: 0.9664 - val_acc: 0.7440\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8344 - acc: 0.7920 - val_loss: 0.9636 - val_acc: 0.7380\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8329 - acc: 0.7912 - val_loss: 0.9713 - val_acc: 0.7400\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8335 - acc: 0.7925 - val_loss: 0.9674 - val_acc: 0.7420\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8329 - acc: 0.7901 - val_loss: 0.9651 - val_acc: 0.7420\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8324 - acc: 0.7919 - val_loss: 0.9673 - val_acc: 0.7340\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8322 - acc: 0.7923 - val_loss: 0.9657 - val_acc: 0.7440\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8356 - acc: 0.7911 - val_loss: 0.9691 - val_acc: 0.7400\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8324 - acc: 0.7928 - val_loss: 0.9716 - val_acc: 0.7440\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8345 - acc: 0.7924 - val_loss: 0.9820 - val_acc: 0.7370\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8341 - acc: 0.7927 - val_loss: 0.9629 - val_acc: 0.7410\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8318 - acc: 0.7921 - val_loss: 1.0045 - val_acc: 0.7280\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8334 - acc: 0.7927 - val_loss: 0.9815 - val_acc: 0.7340\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8328 - acc: 0.7907 - val_loss: 0.9608 - val_acc: 0.7440\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8330 - acc: 0.7933 - val_loss: 0.9611 - val_acc: 0.7450\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8316 - acc: 0.7931 - val_loss: 0.9724 - val_acc: 0.7340\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8330 - acc: 0.7941 - val_loss: 0.9661 - val_acc: 0.7430\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8316 - acc: 0.7911 - val_loss: 0.9696 - val_acc: 0.7430\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8314 - acc: 0.7935 - val_loss: 0.9673 - val_acc: 0.7450\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8311 - acc: 0.7923 - val_loss: 0.9646 - val_acc: 0.7360\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8327 - acc: 0.7916 - val_loss: 0.9663 - val_acc: 0.7390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8309 - acc: 0.7921 - val_loss: 0.9624 - val_acc: 0.7420\n",
      "Epoch 768/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8301 - acc: 0.7907 - val_loss: 0.9679 - val_acc: 0.7410\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8314 - acc: 0.7925 - val_loss: 0.9609 - val_acc: 0.7380\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8296 - acc: 0.7925 - val_loss: 0.9759 - val_acc: 0.7410\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8321 - acc: 0.7921 - val_loss: 0.9677 - val_acc: 0.7370\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8312 - acc: 0.7915 - val_loss: 0.9816 - val_acc: 0.7280\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8294 - acc: 0.7945 - val_loss: 0.9804 - val_acc: 0.7440\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8299 - acc: 0.7944 - val_loss: 0.9670 - val_acc: 0.7370\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8307 - acc: 0.7948 - val_loss: 0.9698 - val_acc: 0.7370\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8301 - acc: 0.7931 - val_loss: 0.9657 - val_acc: 0.7420\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8301 - acc: 0.7924 - val_loss: 0.9621 - val_acc: 0.7380\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8283 - acc: 0.7929 - val_loss: 0.9810 - val_acc: 0.7330\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8320 - acc: 0.7923 - val_loss: 0.9692 - val_acc: 0.7400\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8295 - acc: 0.7952 - val_loss: 0.9852 - val_acc: 0.7270\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8298 - acc: 0.7939 - val_loss: 0.9794 - val_acc: 0.7330\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8289 - acc: 0.7953 - val_loss: 0.9717 - val_acc: 0.7360\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8287 - acc: 0.7941 - val_loss: 0.9736 - val_acc: 0.7440\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8305 - acc: 0.7932 - val_loss: 0.9674 - val_acc: 0.7390\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8288 - acc: 0.7948 - val_loss: 0.9652 - val_acc: 0.7420\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8286 - acc: 0.7935 - val_loss: 0.9657 - val_acc: 0.7410\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8296 - acc: 0.7928 - val_loss: 0.9792 - val_acc: 0.7350\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8289 - acc: 0.7935 - val_loss: 0.9615 - val_acc: 0.7400\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8279 - acc: 0.7969 - val_loss: 0.9653 - val_acc: 0.7440\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8292 - acc: 0.7937 - val_loss: 0.9661 - val_acc: 0.7420\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8285 - acc: 0.7933 - val_loss: 0.9720 - val_acc: 0.7400\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8299 - acc: 0.7941 - val_loss: 0.9748 - val_acc: 0.7360\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8277 - acc: 0.7927 - val_loss: 0.9760 - val_acc: 0.7330\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8270 - acc: 0.7956 - val_loss: 0.9729 - val_acc: 0.7420\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8289 - acc: 0.7917 - val_loss: 0.9771 - val_acc: 0.7340\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8290 - acc: 0.7932 - val_loss: 0.9921 - val_acc: 0.7440\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8273 - acc: 0.7919 - val_loss: 0.9975 - val_acc: 0.7400\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8278 - acc: 0.7939 - val_loss: 0.9658 - val_acc: 0.7390\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8255 - acc: 0.7944 - val_loss: 0.9631 - val_acc: 0.7430\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8248 - acc: 0.7933 - val_loss: 0.9947 - val_acc: 0.7290\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8269 - acc: 0.7955 - val_loss: 0.9753 - val_acc: 0.7390\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8264 - acc: 0.7941 - val_loss: 0.9678 - val_acc: 0.7420\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8263 - acc: 0.7924 - val_loss: 0.9725 - val_acc: 0.7360\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8262 - acc: 0.7957 - val_loss: 0.9692 - val_acc: 0.7380\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8249 - acc: 0.7944 - val_loss: 0.9809 - val_acc: 0.7410\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8300 - acc: 0.7936 - val_loss: 0.9736 - val_acc: 0.7340\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8264 - acc: 0.7923 - val_loss: 0.9601 - val_acc: 0.7450\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8256 - acc: 0.7953 - val_loss: 0.9967 - val_acc: 0.7300\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8261 - acc: 0.7943 - val_loss: 0.9652 - val_acc: 0.7440\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8258 - acc: 0.7924 - val_loss: 0.9731 - val_acc: 0.7360\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8262 - acc: 0.7935 - val_loss: 0.9609 - val_acc: 0.7420\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8257 - acc: 0.7921 - val_loss: 1.0073 - val_acc: 0.7310\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8277 - acc: 0.7945 - val_loss: 0.9704 - val_acc: 0.7430\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8280 - acc: 0.7937 - val_loss: 0.9582 - val_acc: 0.7430\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8234 - acc: 0.7937 - val_loss: 0.9766 - val_acc: 0.7310\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8267 - acc: 0.7936 - val_loss: 0.9619 - val_acc: 0.7440\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8242 - acc: 0.7935 - val_loss: 0.9771 - val_acc: 0.7420\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8235 - acc: 0.7961 - val_loss: 0.9621 - val_acc: 0.7450\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8242 - acc: 0.7959 - val_loss: 0.9664 - val_acc: 0.7360\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8251 - acc: 0.7939 - val_loss: 0.9618 - val_acc: 0.7490\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8229 - acc: 0.7961 - val_loss: 0.9755 - val_acc: 0.7360\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8232 - acc: 0.7955 - val_loss: 0.9634 - val_acc: 0.7460\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8227 - acc: 0.7941 - val_loss: 0.9903 - val_acc: 0.7360\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8230 - acc: 0.7952 - val_loss: 0.9631 - val_acc: 0.7400\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8236 - acc: 0.7947 - val_loss: 0.9619 - val_acc: 0.7390\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8261 - acc: 0.7956 - val_loss: 0.9644 - val_acc: 0.7370\n",
      "Epoch 827/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8226 - acc: 0.7941 - val_loss: 0.9645 - val_acc: 0.7440\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8250 - acc: 0.7953 - val_loss: 0.9626 - val_acc: 0.7390\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8223 - acc: 0.7953 - val_loss: 0.9622 - val_acc: 0.7440\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8224 - acc: 0.7961 - val_loss: 0.9597 - val_acc: 0.7400\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8221 - acc: 0.7965 - val_loss: 0.9677 - val_acc: 0.7480\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8210 - acc: 0.7945 - val_loss: 0.9587 - val_acc: 0.7460\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8207 - acc: 0.7952 - val_loss: 0.9684 - val_acc: 0.7370\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8217 - acc: 0.7964 - val_loss: 0.9603 - val_acc: 0.7400\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8227 - acc: 0.7945 - val_loss: 0.9636 - val_acc: 0.7500\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8208 - acc: 0.7961 - val_loss: 0.9582 - val_acc: 0.7480\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8229 - acc: 0.7951 - val_loss: 0.9858 - val_acc: 0.7390\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8226 - acc: 0.7961 - val_loss: 0.9611 - val_acc: 0.7450\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8214 - acc: 0.7941 - val_loss: 0.9605 - val_acc: 0.7440\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8244 - acc: 0.7955 - val_loss: 0.9600 - val_acc: 0.7460\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8206 - acc: 0.7968 - val_loss: 0.9685 - val_acc: 0.7400\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8209 - acc: 0.7956 - val_loss: 0.9583 - val_acc: 0.7450\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8210 - acc: 0.7965 - val_loss: 0.9858 - val_acc: 0.7340\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8221 - acc: 0.7947 - val_loss: 0.9952 - val_acc: 0.7330\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8211 - acc: 0.7931 - val_loss: 0.9602 - val_acc: 0.7440\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8205 - acc: 0.7945 - val_loss: 0.9630 - val_acc: 0.7430\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8213 - acc: 0.7964 - val_loss: 0.9647 - val_acc: 0.7390\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8199 - acc: 0.7961 - val_loss: 0.9825 - val_acc: 0.7320\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8211 - acc: 0.7952 - val_loss: 0.9672 - val_acc: 0.7360\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8196 - acc: 0.7968 - val_loss: 0.9572 - val_acc: 0.7460\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8179 - acc: 0.7971 - val_loss: 0.9614 - val_acc: 0.7400\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8195 - acc: 0.7956 - val_loss: 0.9773 - val_acc: 0.7320\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8213 - acc: 0.7955 - val_loss: 0.9674 - val_acc: 0.7430\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8195 - acc: 0.7975 - val_loss: 0.9713 - val_acc: 0.7460\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8201 - acc: 0.7945 - val_loss: 0.9604 - val_acc: 0.7430\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8198 - acc: 0.7953 - val_loss: 0.9788 - val_acc: 0.7390\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8183 - acc: 0.7960 - val_loss: 0.9699 - val_acc: 0.7330\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8212 - acc: 0.7967 - val_loss: 0.9765 - val_acc: 0.7380\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8186 - acc: 0.7967 - val_loss: 0.9714 - val_acc: 0.7480\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8199 - acc: 0.7968 - val_loss: 0.9785 - val_acc: 0.7330\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8208 - acc: 0.7957 - val_loss: 0.9819 - val_acc: 0.7290\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8203 - acc: 0.7947 - val_loss: 0.9764 - val_acc: 0.7460\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8186 - acc: 0.7939 - val_loss: 0.9599 - val_acc: 0.7410\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8174 - acc: 0.7969 - val_loss: 0.9601 - val_acc: 0.7390\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8183 - acc: 0.7979 - val_loss: 0.9824 - val_acc: 0.7420\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8201 - acc: 0.7961 - val_loss: 0.9583 - val_acc: 0.7420\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8166 - acc: 0.7961 - val_loss: 0.9760 - val_acc: 0.7370\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8190 - acc: 0.7963 - val_loss: 0.9585 - val_acc: 0.7450\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8163 - acc: 0.7972 - val_loss: 0.9652 - val_acc: 0.7530\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8178 - acc: 0.7973 - val_loss: 0.9604 - val_acc: 0.7370\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8178 - acc: 0.7972 - val_loss: 0.9723 - val_acc: 0.7400\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8181 - acc: 0.7960 - val_loss: 0.9574 - val_acc: 0.7530\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8176 - acc: 0.7969 - val_loss: 0.9723 - val_acc: 0.7440\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8176 - acc: 0.7955 - val_loss: 0.9635 - val_acc: 0.7470\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8190 - acc: 0.7960 - val_loss: 0.9562 - val_acc: 0.7470\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8154 - acc: 0.7971 - val_loss: 0.9588 - val_acc: 0.7440\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8162 - acc: 0.7965 - val_loss: 0.9736 - val_acc: 0.7330\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8175 - acc: 0.7968 - val_loss: 0.9584 - val_acc: 0.7490\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8146 - acc: 0.7979 - val_loss: 0.9651 - val_acc: 0.7440\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8158 - acc: 0.7971 - val_loss: 0.9585 - val_acc: 0.7460\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8165 - acc: 0.7965 - val_loss: 1.0159 - val_acc: 0.7350\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8193 - acc: 0.7968 - val_loss: 0.9579 - val_acc: 0.7390\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8154 - acc: 0.7997 - val_loss: 0.9916 - val_acc: 0.7300\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8154 - acc: 0.7973 - val_loss: 0.9563 - val_acc: 0.7490\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8148 - acc: 0.7977 - val_loss: 0.9787 - val_acc: 0.7460\n",
      "Epoch 886/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8164 - acc: 0.7959 - val_loss: 0.9621 - val_acc: 0.7420\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8153 - acc: 0.7961 - val_loss: 0.9653 - val_acc: 0.7390\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8151 - acc: 0.7981 - val_loss: 0.9664 - val_acc: 0.7330\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8125 - acc: 0.7983 - val_loss: 0.9554 - val_acc: 0.7450\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8165 - acc: 0.7969 - val_loss: 0.9733 - val_acc: 0.7410\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8194 - acc: 0.7933 - val_loss: 0.9566 - val_acc: 0.7420\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8150 - acc: 0.7959 - val_loss: 0.9570 - val_acc: 0.7440\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8131 - acc: 0.7979 - val_loss: 0.9625 - val_acc: 0.7420\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8146 - acc: 0.7968 - val_loss: 0.9543 - val_acc: 0.7440\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8148 - acc: 0.7979 - val_loss: 0.9576 - val_acc: 0.7460\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8138 - acc: 0.7968 - val_loss: 0.9646 - val_acc: 0.7460\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8157 - acc: 0.7976 - val_loss: 0.9829 - val_acc: 0.7340\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8186 - acc: 0.7969 - val_loss: 0.9606 - val_acc: 0.7440\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8118 - acc: 0.7997 - val_loss: 0.9608 - val_acc: 0.7470\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8143 - acc: 0.7976 - val_loss: 0.9728 - val_acc: 0.7360\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8125 - acc: 0.7981 - val_loss: 0.9784 - val_acc: 0.7440\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8152 - acc: 0.7983 - val_loss: 0.9789 - val_acc: 0.7310\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8129 - acc: 0.7956 - val_loss: 0.9576 - val_acc: 0.7430\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8128 - acc: 0.7976 - val_loss: 0.9724 - val_acc: 0.7370\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8108 - acc: 0.7989 - val_loss: 0.9947 - val_acc: 0.7440\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8150 - acc: 0.7976 - val_loss: 0.9643 - val_acc: 0.7410\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8124 - acc: 0.7975 - val_loss: 0.9575 - val_acc: 0.7430\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8127 - acc: 0.8001 - val_loss: 1.0214 - val_acc: 0.7350\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8157 - acc: 0.7968 - val_loss: 0.9742 - val_acc: 0.7360\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8150 - acc: 0.7989 - val_loss: 0.9644 - val_acc: 0.7390\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8129 - acc: 0.8005 - val_loss: 0.9584 - val_acc: 0.7460\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8122 - acc: 0.7987 - val_loss: 0.9548 - val_acc: 0.7470\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8123 - acc: 0.7984 - val_loss: 1.0617 - val_acc: 0.7210\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8186 - acc: 0.7953 - val_loss: 0.9772 - val_acc: 0.7410\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8156 - acc: 0.7993 - val_loss: 0.9557 - val_acc: 0.7480\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8132 - acc: 0.7968 - val_loss: 0.9702 - val_acc: 0.7340\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8123 - acc: 0.7975 - val_loss: 0.9732 - val_acc: 0.7350\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8114 - acc: 0.7991 - val_loss: 0.9718 - val_acc: 0.7370\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8094 - acc: 0.7991 - val_loss: 0.9657 - val_acc: 0.7410\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8131 - acc: 0.7980 - val_loss: 0.9586 - val_acc: 0.7470\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8106 - acc: 0.7984 - val_loss: 1.0282 - val_acc: 0.7320\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8140 - acc: 0.7980 - val_loss: 0.9625 - val_acc: 0.7450\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8116 - acc: 0.7992 - val_loss: 0.9571 - val_acc: 0.7480\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8096 - acc: 0.7992 - val_loss: 0.9538 - val_acc: 0.7410\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8110 - acc: 0.7976 - val_loss: 0.9539 - val_acc: 0.7520\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8094 - acc: 0.7996 - val_loss: 0.9588 - val_acc: 0.7480\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8089 - acc: 0.7988 - val_loss: 0.9605 - val_acc: 0.7410\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8105 - acc: 0.7995 - val_loss: 0.9757 - val_acc: 0.7340\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8121 - acc: 0.7987 - val_loss: 0.9638 - val_acc: 0.7450\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8084 - acc: 0.7996 - val_loss: 0.9660 - val_acc: 0.7460\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8098 - acc: 0.7987 - val_loss: 0.9638 - val_acc: 0.7370\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8092 - acc: 0.7981 - val_loss: 0.9591 - val_acc: 0.7380\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8081 - acc: 0.7996 - val_loss: 0.9542 - val_acc: 0.7470\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8093 - acc: 0.8001 - val_loss: 0.9696 - val_acc: 0.7370\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8094 - acc: 0.7985 - val_loss: 0.9635 - val_acc: 0.7430\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8084 - acc: 0.8004 - val_loss: 0.9596 - val_acc: 0.7450\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8088 - acc: 0.7971 - val_loss: 0.9786 - val_acc: 0.7360\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8100 - acc: 0.7984 - val_loss: 0.9709 - val_acc: 0.7360\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8088 - acc: 0.8024 - val_loss: 1.0022 - val_acc: 0.7350\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8097 - acc: 0.8012 - val_loss: 0.9894 - val_acc: 0.7390\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8097 - acc: 0.8001 - val_loss: 0.9596 - val_acc: 0.7370\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8076 - acc: 0.8004 - val_loss: 0.9531 - val_acc: 0.7440\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8114 - acc: 0.7988 - val_loss: 0.9669 - val_acc: 0.7420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8084 - acc: 0.8028 - val_loss: 0.9610 - val_acc: 0.7360\n",
      "Epoch 945/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8090 - acc: 0.7980 - val_loss: 0.9881 - val_acc: 0.7410\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8092 - acc: 0.7999 - val_loss: 0.9975 - val_acc: 0.7420\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8123 - acc: 0.7985 - val_loss: 0.9646 - val_acc: 0.7430\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8105 - acc: 0.7979 - val_loss: 0.9819 - val_acc: 0.7290\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8070 - acc: 0.8005 - val_loss: 0.9540 - val_acc: 0.7460\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8091 - acc: 0.7981 - val_loss: 0.9557 - val_acc: 0.7390\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8094 - acc: 0.8001 - val_loss: 0.9589 - val_acc: 0.7450\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8079 - acc: 0.8015 - val_loss: 0.9570 - val_acc: 0.7520\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8053 - acc: 0.8001 - val_loss: 0.9597 - val_acc: 0.7440\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8061 - acc: 0.8004 - val_loss: 0.9626 - val_acc: 0.7420\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8065 - acc: 0.8007 - val_loss: 0.9575 - val_acc: 0.7450\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8061 - acc: 0.7991 - val_loss: 0.9659 - val_acc: 0.7380\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8078 - acc: 0.7992 - val_loss: 0.9630 - val_acc: 0.7420\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8065 - acc: 0.7997 - val_loss: 0.9632 - val_acc: 0.7410\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8047 - acc: 0.8008 - val_loss: 0.9637 - val_acc: 0.7420\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8058 - acc: 0.7999 - val_loss: 0.9659 - val_acc: 0.7390\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8053 - acc: 0.7993 - val_loss: 0.9613 - val_acc: 0.7420\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8087 - acc: 0.7999 - val_loss: 0.9509 - val_acc: 0.7470\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8067 - acc: 0.8019 - val_loss: 0.9624 - val_acc: 0.7420\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8055 - acc: 0.8011 - val_loss: 0.9608 - val_acc: 0.7440\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8049 - acc: 0.7992 - val_loss: 0.9930 - val_acc: 0.7320\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8067 - acc: 0.7992 - val_loss: 1.0015 - val_acc: 0.7260\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8062 - acc: 0.8003 - val_loss: 0.9588 - val_acc: 0.7460\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8073 - acc: 0.8009 - val_loss: 0.9510 - val_acc: 0.7480\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8042 - acc: 0.8001 - val_loss: 0.9550 - val_acc: 0.7400\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8076 - acc: 0.7997 - val_loss: 0.9559 - val_acc: 0.7450\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8064 - acc: 0.7999 - val_loss: 0.9531 - val_acc: 0.7460\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8051 - acc: 0.8008 - val_loss: 0.9550 - val_acc: 0.7420\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8046 - acc: 0.7989 - val_loss: 0.9527 - val_acc: 0.7530\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8064 - acc: 0.7995 - val_loss: 0.9632 - val_acc: 0.7410\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8045 - acc: 0.8019 - val_loss: 0.9715 - val_acc: 0.7350\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8049 - acc: 0.8012 - val_loss: 0.9626 - val_acc: 0.7340\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8036 - acc: 0.8017 - val_loss: 0.9546 - val_acc: 0.7500\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8091 - acc: 0.7993 - val_loss: 0.9527 - val_acc: 0.7460\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8042 - acc: 0.8005 - val_loss: 0.9847 - val_acc: 0.7380\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8046 - acc: 0.7977 - val_loss: 0.9586 - val_acc: 0.7450\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8026 - acc: 0.7992 - val_loss: 0.9882 - val_acc: 0.7340\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8085 - acc: 0.7984 - val_loss: 0.9915 - val_acc: 0.7320\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8048 - acc: 0.8000 - val_loss: 0.9652 - val_acc: 0.7400\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8061 - acc: 0.7993 - val_loss: 0.9626 - val_acc: 0.7450\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8044 - acc: 0.8000 - val_loss: 0.9529 - val_acc: 0.7470\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8033 - acc: 0.8003 - val_loss: 0.9569 - val_acc: 0.7390\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8016 - acc: 0.8028 - val_loss: 0.9543 - val_acc: 0.7420\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8024 - acc: 0.8023 - val_loss: 1.0143 - val_acc: 0.7230\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8044 - acc: 0.8009 - val_loss: 0.9649 - val_acc: 0.7460\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8030 - acc: 0.8016 - val_loss: 0.9564 - val_acc: 0.7450\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8022 - acc: 0.8035 - val_loss: 0.9610 - val_acc: 0.7370\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8044 - acc: 0.8003 - val_loss: 0.9518 - val_acc: 0.7460\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8017 - acc: 0.8013 - val_loss: 0.9543 - val_acc: 0.7460\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8020 - acc: 0.8007 - val_loss: 0.9732 - val_acc: 0.7440\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8016 - acc: 0.7989 - val_loss: 0.9505 - val_acc: 0.7470\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8012 - acc: 0.8021 - val_loss: 0.9492 - val_acc: 0.7460\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8031 - acc: 0.8001 - val_loss: 0.9632 - val_acc: 0.7410\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8050 - acc: 0.8019 - val_loss: 0.9533 - val_acc: 0.7450\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8015 - acc: 0.8017 - val_loss: 0.9667 - val_acc: 0.7370\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8012 - acc: 0.8024 - val_loss: 0.9515 - val_acc: 0.7490\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"36031ee9-3738-4879-a27a-8df4343236b3\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"36031ee9-3738-4879-a27a-8df4343236b3\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,)))\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'g,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 40us/step\n",
      "1500/1500 [==============================] - 0s 39us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7928270435969035, 0.8067999999682108]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8838497360547384, 0.7700000003178914]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 1s 74us/step - loss: 1.9615 - acc: 0.1701 - val_loss: 1.9340 - val_acc: 0.1870\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.9449 - acc: 0.1745 - val_loss: 1.9223 - val_acc: 0.2050\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.9316 - acc: 0.1896 - val_loss: 1.9127 - val_acc: 0.2120\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9183 - acc: 0.2081 - val_loss: 1.9021 - val_acc: 0.2260\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.9116 - acc: 0.2067 - val_loss: 1.8915 - val_acc: 0.2380\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.8955 - acc: 0.2173 - val_loss: 1.8790 - val_acc: 0.2430\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8884 - acc: 0.2128 - val_loss: 1.8658 - val_acc: 0.2500\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8796 - acc: 0.2245 - val_loss: 1.8522 - val_acc: 0.2640\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 1.8666 - acc: 0.2364 - val_loss: 1.8364 - val_acc: 0.2750\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.8526 - acc: 0.2427 - val_loss: 1.8183 - val_acc: 0.3000\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8358 - acc: 0.2587 - val_loss: 1.7988 - val_acc: 0.3150\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.8214 - acc: 0.2625 - val_loss: 1.7769 - val_acc: 0.3260\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.8056 - acc: 0.2727 - val_loss: 1.7541 - val_acc: 0.3510\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.7947 - acc: 0.2799 - val_loss: 1.7309 - val_acc: 0.3650\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7740 - acc: 0.2957 - val_loss: 1.7075 - val_acc: 0.3770\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7615 - acc: 0.3004 - val_loss: 1.6859 - val_acc: 0.3890\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 1.7381 - acc: 0.3141 - val_loss: 1.6614 - val_acc: 0.4000\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.7214 - acc: 0.3160 - val_loss: 1.6387 - val_acc: 0.4090\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7069 - acc: 0.3269 - val_loss: 1.6153 - val_acc: 0.4220\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6866 - acc: 0.3388 - val_loss: 1.5917 - val_acc: 0.4330\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.6662 - acc: 0.3480 - val_loss: 1.5682 - val_acc: 0.4470\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.6630 - acc: 0.3400 - val_loss: 1.5471 - val_acc: 0.4540\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 45us/step - loss: 1.6407 - acc: 0.3481 - val_loss: 1.5249 - val_acc: 0.4850\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.6145 - acc: 0.3619 - val_loss: 1.5013 - val_acc: 0.4910\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.6032 - acc: 0.3723 - val_loss: 1.4815 - val_acc: 0.5100\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.5914 - acc: 0.3744 - val_loss: 1.4618 - val_acc: 0.5310\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5806 - acc: 0.3800 - val_loss: 1.4416 - val_acc: 0.5430\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.5576 - acc: 0.3956 - val_loss: 1.4196 - val_acc: 0.5550\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.5425 - acc: 0.4031 - val_loss: 1.3993 - val_acc: 0.5650\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.5330 - acc: 0.3985 - val_loss: 1.3805 - val_acc: 0.5850\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.5224 - acc: 0.4123 - val_loss: 1.3611 - val_acc: 0.5930\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4953 - acc: 0.4157 - val_loss: 1.3420 - val_acc: 0.6080\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.4848 - acc: 0.4285 - val_loss: 1.3228 - val_acc: 0.6090\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.4796 - acc: 0.4331 - val_loss: 1.3081 - val_acc: 0.6230\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4524 - acc: 0.4421 - val_loss: 1.2855 - val_acc: 0.6260\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.4558 - acc: 0.4311 - val_loss: 1.2691 - val_acc: 0.6310\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4336 - acc: 0.4500 - val_loss: 1.2511 - val_acc: 0.6350\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.4178 - acc: 0.4591 - val_loss: 1.2341 - val_acc: 0.6440\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3956 - acc: 0.4773 - val_loss: 1.2158 - val_acc: 0.6490\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3945 - acc: 0.4715 - val_loss: 1.2021 - val_acc: 0.6560\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3702 - acc: 0.4813 - val_loss: 1.1834 - val_acc: 0.6600\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.3726 - acc: 0.4756 - val_loss: 1.1685 - val_acc: 0.6610\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.3571 - acc: 0.4879 - val_loss: 1.1556 - val_acc: 0.6720\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3331 - acc: 0.4921 - val_loss: 1.1383 - val_acc: 0.6780\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3300 - acc: 0.4955 - val_loss: 1.1236 - val_acc: 0.6750\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.3230 - acc: 0.4991 - val_loss: 1.1116 - val_acc: 0.6850\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.3039 - acc: 0.5104 - val_loss: 1.0969 - val_acc: 0.6860\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2966 - acc: 0.5091 - val_loss: 1.0829 - val_acc: 0.6840\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2741 - acc: 0.5172 - val_loss: 1.0701 - val_acc: 0.6880\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2670 - acc: 0.5251 - val_loss: 1.0573 - val_acc: 0.6950\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2731 - acc: 0.5233 - val_loss: 1.0452 - val_acc: 0.6950\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2582 - acc: 0.5300 - val_loss: 1.0355 - val_acc: 0.6960\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.2358 - acc: 0.5367 - val_loss: 1.0207 - val_acc: 0.7050\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2294 - acc: 0.5377 - val_loss: 1.0111 - val_acc: 0.7000\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.2237 - acc: 0.5408 - val_loss: 1.0006 - val_acc: 0.7030\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.2171 - acc: 0.5448 - val_loss: 0.9899 - val_acc: 0.7060\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1983 - acc: 0.5492 - val_loss: 0.9805 - val_acc: 0.7070\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.2022 - acc: 0.5551 - val_loss: 0.9721 - val_acc: 0.7090\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1885 - acc: 0.5556 - val_loss: 0.9616 - val_acc: 0.7110\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 39us/step - loss: 1.1788 - acc: 0.5624 - val_loss: 0.9528 - val_acc: 0.7130\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1744 - acc: 0.5625 - val_loss: 0.9435 - val_acc: 0.7120\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1537 - acc: 0.5644 - val_loss: 0.9375 - val_acc: 0.7210\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1546 - acc: 0.5637 - val_loss: 0.9305 - val_acc: 0.7160\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1313 - acc: 0.5827 - val_loss: 0.9197 - val_acc: 0.7230\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1333 - acc: 0.5793 - val_loss: 0.9121 - val_acc: 0.7220\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1382 - acc: 0.5849 - val_loss: 0.9059 - val_acc: 0.7200\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1242 - acc: 0.5899 - val_loss: 0.8992 - val_acc: 0.7260\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1160 - acc: 0.5849 - val_loss: 0.8904 - val_acc: 0.7260\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.1137 - acc: 0.5916 - val_loss: 0.8854 - val_acc: 0.7270\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.1065 - acc: 0.5861 - val_loss: 0.8804 - val_acc: 0.7310\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 1.1007 - acc: 0.5929 - val_loss: 0.8731 - val_acc: 0.7250\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0949 - acc: 0.5965 - val_loss: 0.8684 - val_acc: 0.7310\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0919 - acc: 0.5945 - val_loss: 0.8618 - val_acc: 0.7320\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0786 - acc: 0.5977 - val_loss: 0.8562 - val_acc: 0.7340\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0721 - acc: 0.6011 - val_loss: 0.8509 - val_acc: 0.7350\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0686 - acc: 0.6035 - val_loss: 0.8443 - val_acc: 0.7340\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0633 - acc: 0.6057 - val_loss: 0.8388 - val_acc: 0.7380\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0451 - acc: 0.6145 - val_loss: 0.8328 - val_acc: 0.7350\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0434 - acc: 0.6143 - val_loss: 0.8271 - val_acc: 0.7380\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0384 - acc: 0.6192 - val_loss: 0.8220 - val_acc: 0.7340\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0296 - acc: 0.6189 - val_loss: 0.8183 - val_acc: 0.7350\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0342 - acc: 0.6161 - val_loss: 0.8137 - val_acc: 0.7390\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0288 - acc: 0.6160 - val_loss: 0.8103 - val_acc: 0.7390\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0446 - acc: 0.6127 - val_loss: 0.8099 - val_acc: 0.7380\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0251 - acc: 0.6256 - val_loss: 0.8063 - val_acc: 0.7400\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0076 - acc: 0.6212 - val_loss: 0.8017 - val_acc: 0.7410\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0095 - acc: 0.6279 - val_loss: 0.7972 - val_acc: 0.7370\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0046 - acc: 0.6212 - val_loss: 0.7912 - val_acc: 0.7390\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0018 - acc: 0.6272 - val_loss: 0.7883 - val_acc: 0.7400\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.0075 - acc: 0.6217 - val_loss: 0.7869 - val_acc: 0.7380\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9984 - acc: 0.6252 - val_loss: 0.7841 - val_acc: 0.7430\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9755 - acc: 0.6353 - val_loss: 0.7789 - val_acc: 0.7400\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9713 - acc: 0.6420 - val_loss: 0.7742 - val_acc: 0.7380\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9707 - acc: 0.6351 - val_loss: 0.7722 - val_acc: 0.7410\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9794 - acc: 0.6392 - val_loss: 0.7689 - val_acc: 0.7420\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9740 - acc: 0.6428 - val_loss: 0.7673 - val_acc: 0.7400\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9637 - acc: 0.6436 - val_loss: 0.7622 - val_acc: 0.7410\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9601 - acc: 0.6459 - val_loss: 0.7599 - val_acc: 0.7410\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9645 - acc: 0.6364 - val_loss: 0.7586 - val_acc: 0.7390\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9490 - acc: 0.6512 - val_loss: 0.7557 - val_acc: 0.7410\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9517 - acc: 0.6512 - val_loss: 0.7553 - val_acc: 0.7400\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9525 - acc: 0.6447 - val_loss: 0.7510 - val_acc: 0.7400\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.9618 - acc: 0.6405 - val_loss: 0.7500 - val_acc: 0.7440\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9489 - acc: 0.6492 - val_loss: 0.7475 - val_acc: 0.7430\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.9550 - acc: 0.6349 - val_loss: 0.7440 - val_acc: 0.7430\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9269 - acc: 0.6625 - val_loss: 0.7406 - val_acc: 0.7420\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9247 - acc: 0.6573 - val_loss: 0.7365 - val_acc: 0.7450\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9314 - acc: 0.6584 - val_loss: 0.7349 - val_acc: 0.7430\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9277 - acc: 0.6551 - val_loss: 0.7353 - val_acc: 0.7460\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9124 - acc: 0.6620 - val_loss: 0.7319 - val_acc: 0.7440\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.9227 - acc: 0.6640 - val_loss: 0.7312 - val_acc: 0.7480\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9261 - acc: 0.6580 - val_loss: 0.7275 - val_acc: 0.7460\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.9057 - acc: 0.6604 - val_loss: 0.7258 - val_acc: 0.7440\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8988 - acc: 0.6645 - val_loss: 0.7226 - val_acc: 0.7430\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8962 - acc: 0.6689 - val_loss: 0.7213 - val_acc: 0.7440\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.9083 - acc: 0.6623 - val_loss: 0.7219 - val_acc: 0.7460\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8908 - acc: 0.6695 - val_loss: 0.7179 - val_acc: 0.7450\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8857 - acc: 0.6736 - val_loss: 0.7157 - val_acc: 0.7440\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8914 - acc: 0.6713 - val_loss: 0.7136 - val_acc: 0.7440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8930 - acc: 0.6667 - val_loss: 0.7136 - val_acc: 0.7490\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8939 - acc: 0.6676 - val_loss: 0.7122 - val_acc: 0.7420\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8849 - acc: 0.6729 - val_loss: 0.7088 - val_acc: 0.7480\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8896 - acc: 0.6785 - val_loss: 0.7085 - val_acc: 0.7460\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8888 - acc: 0.6755 - val_loss: 0.7072 - val_acc: 0.7450\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8783 - acc: 0.6787 - val_loss: 0.7070 - val_acc: 0.7440\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8763 - acc: 0.6739 - val_loss: 0.7044 - val_acc: 0.7470\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8774 - acc: 0.6741 - val_loss: 0.7048 - val_acc: 0.7440\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8783 - acc: 0.6669 - val_loss: 0.6995 - val_acc: 0.7470\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.8582 - acc: 0.6805 - val_loss: 0.6988 - val_acc: 0.7460\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8637 - acc: 0.6759 - val_loss: 0.6957 - val_acc: 0.7470\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8546 - acc: 0.6844 - val_loss: 0.6958 - val_acc: 0.7450\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8587 - acc: 0.6764 - val_loss: 0.6948 - val_acc: 0.7450\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8477 - acc: 0.6813 - val_loss: 0.6935 - val_acc: 0.7460\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8662 - acc: 0.6852 - val_loss: 0.6924 - val_acc: 0.7450\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8464 - acc: 0.6857 - val_loss: 0.6903 - val_acc: 0.7440\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8495 - acc: 0.6813 - val_loss: 0.6884 - val_acc: 0.7480\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8412 - acc: 0.6837 - val_loss: 0.6875 - val_acc: 0.7470\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8499 - acc: 0.6903 - val_loss: 0.6871 - val_acc: 0.7480\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8442 - acc: 0.6880 - val_loss: 0.6876 - val_acc: 0.7470\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8381 - acc: 0.6957 - val_loss: 0.6853 - val_acc: 0.7450\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8203 - acc: 0.7000 - val_loss: 0.6832 - val_acc: 0.7460\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8453 - acc: 0.6825 - val_loss: 0.6825 - val_acc: 0.7470\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8284 - acc: 0.6960 - val_loss: 0.6793 - val_acc: 0.7490\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8200 - acc: 0.6997 - val_loss: 0.6777 - val_acc: 0.7480\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8240 - acc: 0.6940 - val_loss: 0.6780 - val_acc: 0.7460\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 48us/step - loss: 0.8211 - acc: 0.6949 - val_loss: 0.6751 - val_acc: 0.7460\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8324 - acc: 0.6904 - val_loss: 0.6781 - val_acc: 0.7460\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 0.8338 - acc: 0.6883 - val_loss: 0.6767 - val_acc: 0.7490\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.8188 - acc: 0.6967 - val_loss: 0.6751 - val_acc: 0.7480\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8278 - acc: 0.6901 - val_loss: 0.6731 - val_acc: 0.7480\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8021 - acc: 0.6963 - val_loss: 0.6710 - val_acc: 0.7490\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.8162 - acc: 0.7001 - val_loss: 0.6713 - val_acc: 0.7470\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8099 - acc: 0.6975 - val_loss: 0.6713 - val_acc: 0.7490\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8208 - acc: 0.6972 - val_loss: 0.6703 - val_acc: 0.7490\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.8078 - acc: 0.6961 - val_loss: 0.6686 - val_acc: 0.7480\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8110 - acc: 0.7021 - val_loss: 0.6680 - val_acc: 0.7490\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8018 - acc: 0.7012 - val_loss: 0.6672 - val_acc: 0.7480\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8071 - acc: 0.6972 - val_loss: 0.6661 - val_acc: 0.7480\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7962 - acc: 0.7013 - val_loss: 0.6655 - val_acc: 0.7460\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7946 - acc: 0.7061 - val_loss: 0.6642 - val_acc: 0.7470\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7952 - acc: 0.7028 - val_loss: 0.6632 - val_acc: 0.7490\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7988 - acc: 0.6957 - val_loss: 0.6639 - val_acc: 0.7490\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.8028 - acc: 0.7073 - val_loss: 0.6627 - val_acc: 0.7480\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7949 - acc: 0.7023 - val_loss: 0.6611 - val_acc: 0.7510\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7864 - acc: 0.7093 - val_loss: 0.6608 - val_acc: 0.7510\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7887 - acc: 0.7080 - val_loss: 0.6599 - val_acc: 0.7510\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7882 - acc: 0.7056 - val_loss: 0.6599 - val_acc: 0.7480\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7834 - acc: 0.7068 - val_loss: 0.6591 - val_acc: 0.7480\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7761 - acc: 0.7159 - val_loss: 0.6570 - val_acc: 0.7490\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7882 - acc: 0.7037 - val_loss: 0.6567 - val_acc: 0.7520\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7719 - acc: 0.7113 - val_loss: 0.6553 - val_acc: 0.7500\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7863 - acc: 0.7080 - val_loss: 0.6553 - val_acc: 0.7520\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7738 - acc: 0.7133 - val_loss: 0.6547 - val_acc: 0.7470\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7675 - acc: 0.7147 - val_loss: 0.6531 - val_acc: 0.7530\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7819 - acc: 0.7047 - val_loss: 0.6540 - val_acc: 0.7510\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7740 - acc: 0.7128 - val_loss: 0.6532 - val_acc: 0.7510\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7696 - acc: 0.7179 - val_loss: 0.6517 - val_acc: 0.7530\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7651 - acc: 0.7127 - val_loss: 0.6503 - val_acc: 0.7490\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7566 - acc: 0.7145 - val_loss: 0.6479 - val_acc: 0.7500\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7507 - acc: 0.7161 - val_loss: 0.6479 - val_acc: 0.7510\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7521 - acc: 0.7180 - val_loss: 0.6480 - val_acc: 0.7460\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7566 - acc: 0.7184 - val_loss: 0.6465 - val_acc: 0.7500\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7705 - acc: 0.7131 - val_loss: 0.6449 - val_acc: 0.7550\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7707 - acc: 0.7169 - val_loss: 0.6468 - val_acc: 0.7520\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7538 - acc: 0.7217 - val_loss: 0.6483 - val_acc: 0.7470\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7624 - acc: 0.7109 - val_loss: 0.6480 - val_acc: 0.7500\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.7524 - acc: 0.7233 - val_loss: 0.6480 - val_acc: 0.7480\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7489 - acc: 0.7193 - val_loss: 0.6480 - val_acc: 0.7470\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7489 - acc: 0.7192 - val_loss: 0.6454 - val_acc: 0.7500\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7417 - acc: 0.7277 - val_loss: 0.6441 - val_acc: 0.7520\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7417 - acc: 0.7215 - val_loss: 0.6437 - val_acc: 0.7500\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7514 - acc: 0.7173 - val_loss: 0.6424 - val_acc: 0.7510\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7462 - acc: 0.7207 - val_loss: 0.6411 - val_acc: 0.7500\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7465 - acc: 0.7197 - val_loss: 0.6409 - val_acc: 0.7480\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7471 - acc: 0.7223 - val_loss: 0.6395 - val_acc: 0.7510\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 0.7398 - acc: 0.7255 - val_loss: 0.6398 - val_acc: 0.7510\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7370 - acc: 0.7245 - val_loss: 0.6379 - val_acc: 0.7500\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 0.7393 - acc: 0.7180 - val_loss: 0.6388 - val_acc: 0.7510\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7192 - acc: 0.7273 - val_loss: 0.6380 - val_acc: 0.7490\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 0.7292 - acc: 0.7239 - val_loss: 0.6386 - val_acc: 0.7500\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"7a7f5f4a-af2e-4d68-8818-77d57d498e9f\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"7a7f5f4a-af2e-4d68-8818-77d57d498e9f\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 47us/step\n",
      "1500/1500 [==============================] - 0s 48us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.45620859278043113, 0.8326666666348775]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5957788271903992, 0.7773333336512248]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"26c0b1f4-c08a-4956-be32-a4e4feeb9dd6\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"26c0b1f4-c08a-4956-be32-a4e4feeb9dd6\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]\n",
    "\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 36us/step - loss: 1.8761 - acc: 0.2351 - val_loss: 1.8030 - val_acc: 0.3050\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 1.7273 - acc: 0.3612 - val_loss: 1.6333 - val_acc: 0.4180\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 1.5521 - acc: 0.4672 - val_loss: 1.4557 - val_acc: 0.5090\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 1.3766 - acc: 0.5524 - val_loss: 1.2864 - val_acc: 0.5850\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 1.2161 - acc: 0.6160 - val_loss: 1.1413 - val_acc: 0.6303\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 1.0805 - acc: 0.6582 - val_loss: 1.0231 - val_acc: 0.6623\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.9711 - acc: 0.6865 - val_loss: 0.9305 - val_acc: 0.6883\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.8861 - acc: 0.7055 - val_loss: 0.8608 - val_acc: 0.7093\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.8210 - acc: 0.7194 - val_loss: 0.8077 - val_acc: 0.7203\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.7717 - acc: 0.7302 - val_loss: 0.7685 - val_acc: 0.7307\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.7336 - acc: 0.7404 - val_loss: 0.7390 - val_acc: 0.7337\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.7032 - acc: 0.7489 - val_loss: 0.7151 - val_acc: 0.7400\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.6790 - acc: 0.7560 - val_loss: 0.6965 - val_acc: 0.7420\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.6586 - acc: 0.7622 - val_loss: 0.6814 - val_acc: 0.7473\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.6414 - acc: 0.7671 - val_loss: 0.6693 - val_acc: 0.7530\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.6263 - acc: 0.7725 - val_loss: 0.6584 - val_acc: 0.7563\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.6133 - acc: 0.7764 - val_loss: 0.6500 - val_acc: 0.7620\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.6015 - acc: 0.7812 - val_loss: 0.6423 - val_acc: 0.7667\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.5906 - acc: 0.7851 - val_loss: 0.6346 - val_acc: 0.7637\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.5809 - acc: 0.7881 - val_loss: 0.6291 - val_acc: 0.7660\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.5716 - acc: 0.7928 - val_loss: 0.6228 - val_acc: 0.7663\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.5636 - acc: 0.7945 - val_loss: 0.6163 - val_acc: 0.7713\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.5558 - acc: 0.7981 - val_loss: 0.6131 - val_acc: 0.7723\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.5482 - acc: 0.8004 - val_loss: 0.6076 - val_acc: 0.7750\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.5415 - acc: 0.8042 - val_loss: 0.6038 - val_acc: 0.7710\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.5346 - acc: 0.8064 - val_loss: 0.6004 - val_acc: 0.7770\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.5286 - acc: 0.8088 - val_loss: 0.5969 - val_acc: 0.7787\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.5226 - acc: 0.8112 - val_loss: 0.5933 - val_acc: 0.7780\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.5167 - acc: 0.8148 - val_loss: 0.5906 - val_acc: 0.7823\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.5114 - acc: 0.8168 - val_loss: 0.5877 - val_acc: 0.7830\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.5059 - acc: 0.8185 - val_loss: 0.5865 - val_acc: 0.7847\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.5011 - acc: 0.8212 - val_loss: 0.5817 - val_acc: 0.7873\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4960 - acc: 0.8221 - val_loss: 0.5805 - val_acc: 0.7867\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4912 - acc: 0.8251 - val_loss: 0.5785 - val_acc: 0.7897\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4867 - acc: 0.8260 - val_loss: 0.5766 - val_acc: 0.7917\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.4824 - acc: 0.8285 - val_loss: 0.5739 - val_acc: 0.7920\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.4780 - acc: 0.8305 - val_loss: 0.5722 - val_acc: 0.7897\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4738 - acc: 0.8319 - val_loss: 0.5729 - val_acc: 0.7920\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.4701 - acc: 0.8325 - val_loss: 0.5708 - val_acc: 0.7910\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4663 - acc: 0.8350 - val_loss: 0.5686 - val_acc: 0.7927\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.4624 - acc: 0.8358 - val_loss: 0.5662 - val_acc: 0.7927\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.4587 - acc: 0.8375 - val_loss: 0.5649 - val_acc: 0.7947\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4553 - acc: 0.8393 - val_loss: 0.5643 - val_acc: 0.7943\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4516 - acc: 0.8406 - val_loss: 0.5654 - val_acc: 0.7947\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4482 - acc: 0.8406 - val_loss: 0.5644 - val_acc: 0.7917\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4449 - acc: 0.8433 - val_loss: 0.5627 - val_acc: 0.7920\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.4419 - acc: 0.8436 - val_loss: 0.5616 - val_acc: 0.7950\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.4388 - acc: 0.8456 - val_loss: 0.5614 - val_acc: 0.7953\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.4360 - acc: 0.8460 - val_loss: 0.5587 - val_acc: 0.7960\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.4328 - acc: 0.8464 - val_loss: 0.5617 - val_acc: 0.7953\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.4302 - acc: 0.8495 - val_loss: 0.5582 - val_acc: 0.7970\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.4271 - acc: 0.8496 - val_loss: 0.5572 - val_acc: 0.7983\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.4243 - acc: 0.8510 - val_loss: 0.5558 - val_acc: 0.7987\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.4217 - acc: 0.8522 - val_loss: 0.5565 - val_acc: 0.7963\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4192 - acc: 0.8524 - val_loss: 0.5555 - val_acc: 0.7987\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4167 - acc: 0.8533 - val_loss: 0.5568 - val_acc: 0.7980\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4140 - acc: 0.8548 - val_loss: 0.5555 - val_acc: 0.7963\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4118 - acc: 0.8550 - val_loss: 0.5568 - val_acc: 0.7993\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4092 - acc: 0.8566 - val_loss: 0.5567 - val_acc: 0.7987\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.4071 - acc: 0.8565 - val_loss: 0.5571 - val_acc: 0.7963\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4045 - acc: 0.8579 - val_loss: 0.5556 - val_acc: 0.7977\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.4020 - acc: 0.8594 - val_loss: 0.5556 - val_acc: 0.7977\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3999 - acc: 0.8593 - val_loss: 0.5564 - val_acc: 0.7987\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3976 - acc: 0.8591 - val_loss: 0.5563 - val_acc: 0.7977\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3955 - acc: 0.8613 - val_loss: 0.5540 - val_acc: 0.7977\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3934 - acc: 0.8613 - val_loss: 0.5547 - val_acc: 0.7993\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3914 - acc: 0.8635 - val_loss: 0.5547 - val_acc: 0.7997\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3893 - acc: 0.8636 - val_loss: 0.5567 - val_acc: 0.8000\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3873 - acc: 0.8635 - val_loss: 0.5570 - val_acc: 0.8007\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3855 - acc: 0.8655 - val_loss: 0.5540 - val_acc: 0.7997\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3835 - acc: 0.8659 - val_loss: 0.5575 - val_acc: 0.7960\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3815 - acc: 0.8676 - val_loss: 0.5555 - val_acc: 0.8003\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3798 - acc: 0.8669 - val_loss: 0.5555 - val_acc: 0.7980\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3778 - acc: 0.8675 - val_loss: 0.5552 - val_acc: 0.8020\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3761 - acc: 0.8691 - val_loss: 0.5560 - val_acc: 0.7987\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3740 - acc: 0.8699 - val_loss: 0.5559 - val_acc: 0.7987\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3724 - acc: 0.8700 - val_loss: 0.5586 - val_acc: 0.7980\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3706 - acc: 0.8706 - val_loss: 0.5583 - val_acc: 0.7980\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3686 - acc: 0.8715 - val_loss: 0.5602 - val_acc: 0.7993\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3669 - acc: 0.8722 - val_loss: 0.5601 - val_acc: 0.8010\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3656 - acc: 0.8730 - val_loss: 0.5585 - val_acc: 0.7977\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3638 - acc: 0.8736 - val_loss: 0.5584 - val_acc: 0.8010\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3622 - acc: 0.8748 - val_loss: 0.5591 - val_acc: 0.7977\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3604 - acc: 0.8747 - val_loss: 0.5601 - val_acc: 0.7997\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3590 - acc: 0.8745 - val_loss: 0.5605 - val_acc: 0.7997\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3571 - acc: 0.8762 - val_loss: 0.5618 - val_acc: 0.7980\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3558 - acc: 0.8762 - val_loss: 0.5647 - val_acc: 0.7953\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3541 - acc: 0.8770 - val_loss: 0.5633 - val_acc: 0.7953\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3527 - acc: 0.8772 - val_loss: 0.5637 - val_acc: 0.7973\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3511 - acc: 0.8775 - val_loss: 0.5619 - val_acc: 0.7983\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3498 - acc: 0.8790 - val_loss: 0.5646 - val_acc: 0.7997\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.3482 - acc: 0.8796 - val_loss: 0.5641 - val_acc: 0.7990\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 28us/step - loss: 0.3465 - acc: 0.8790 - val_loss: 0.5693 - val_acc: 0.8030\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3453 - acc: 0.8803 - val_loss: 0.5674 - val_acc: 0.7970\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3438 - acc: 0.8806 - val_loss: 0.5652 - val_acc: 0.7980\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3425 - acc: 0.8805 - val_loss: 0.5665 - val_acc: 0.8007\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3412 - acc: 0.8806 - val_loss: 0.5705 - val_acc: 0.7960\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3398 - acc: 0.8815 - val_loss: 0.5691 - val_acc: 0.8017\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3386 - acc: 0.8822 - val_loss: 0.5714 - val_acc: 0.7947\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.3372 - acc: 0.8817 - val_loss: 0.5711 - val_acc: 0.7963\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3355 - acc: 0.8832 - val_loss: 0.5696 - val_acc: 0.7947\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3343 - acc: 0.8842 - val_loss: 0.5732 - val_acc: 0.7953\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3331 - acc: 0.8838 - val_loss: 0.5739 - val_acc: 0.7960\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3316 - acc: 0.8848 - val_loss: 0.5730 - val_acc: 0.7953\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3308 - acc: 0.8849 - val_loss: 0.5739 - val_acc: 0.7967\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3292 - acc: 0.8852 - val_loss: 0.5730 - val_acc: 0.7967\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3279 - acc: 0.8864 - val_loss: 0.5780 - val_acc: 0.7950\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3269 - acc: 0.8862 - val_loss: 0.5753 - val_acc: 0.7950\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3255 - acc: 0.8865 - val_loss: 0.5771 - val_acc: 0.7970\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3245 - acc: 0.8864 - val_loss: 0.5800 - val_acc: 0.7967\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3233 - acc: 0.8870 - val_loss: 0.5781 - val_acc: 0.7960\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.3219 - acc: 0.8872 - val_loss: 0.5837 - val_acc: 0.7937\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3205 - acc: 0.8884 - val_loss: 0.5815 - val_acc: 0.7963\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.3195 - acc: 0.8892 - val_loss: 0.5817 - val_acc: 0.7957\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 26us/step - loss: 0.3184 - acc: 0.8887 - val_loss: 0.5824 - val_acc: 0.7943\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 27us/step - loss: 0.3168 - acc: 0.8902 - val_loss: 0.5898 - val_acc: 0.7940\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3162 - acc: 0.8897 - val_loss: 0.5851 - val_acc: 0.7937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3148 - acc: 0.8907 - val_loss: 0.5847 - val_acc: 0.7990\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 24us/step - loss: 0.3135 - acc: 0.8900 - val_loss: 0.5850 - val_acc: 0.7977\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 25us/step - loss: 0.3126 - acc: 0.8911 - val_loss: 0.5891 - val_acc: 0.7963\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"7b204eeb-d4eb-4157-9838-8b47029f5e1e\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"7b204eeb-d4eb-4157-9838-8b47029f5e1e\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))\n",
    "\n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 38us/step\n",
      "4000/4000 [==============================] - 0s 57us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.30771879800883206, 0.8938181818181818]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.59666898599267, 0.80275]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
